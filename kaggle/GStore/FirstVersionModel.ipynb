{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape (903653, 12)\n",
      "test.shape (804684, 12)\n",
      "\n",
      "process_date done\n",
      "process_device done\n",
      "process_geo done\n",
      "process_totals done\n",
      "process_last_time done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:153: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:154: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_transaction_count done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab0cc8c395743b49727a68b92293a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188747dea2b4fe68f4954a65a4dec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=714167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03dcc3cc11843a49d3e310a101d7dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e80c4c502a49d292a2cb2b770864eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=617242), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "process_buy_times done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as bar;\n",
    "\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/GStore/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "train = pd.read_csv(dir + train_file, low_memory=False)\n",
    "test = pd.read_csv(dir + test_file, low_memory=False)\n",
    "\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "cate_features = []\n",
    "numeric_features = []\n",
    "\n",
    "train['fullVisitorId'] = train['fullVisitorId'].astype(str)\n",
    "test['fullVisitorId'] = test['fullVisitorId'].astype(str)\n",
    "\n",
    "train['channelGrouping'] = label.fit_transform(train['channelGrouping'])\n",
    "test['channelGrouping'] = label.fit_transform(test['channelGrouping'])\n",
    "\n",
    "cate_features.append('channelGrouping')\n",
    "\n",
    "def label_transform(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = label.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "####################################\n",
    "date_features = ['date','year','month','day','week','weekofyear','dayofweek','quarter','month_start','month_end']\n",
    "\n",
    "def process_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d',errors='ignore')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month_start'] = df['date'].dt.is_month_start\n",
    "    df['month_end'] = df['date'].dt.is_month_end\n",
    "\n",
    "    df = label_transform(df, date_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_date(train)\n",
    "test = process_date(test)\n",
    "cate_features += date_features\n",
    "print('process_date done')\n",
    "\n",
    "\n",
    "################device####################\n",
    "device_features = ['browser','operatingSystem','isMobile','deviceCategory']\n",
    "\n",
    "def process_device(df):\n",
    "    df['browser'] = df['device'].progress_apply(lambda x: json.loads(x)['browser'])\n",
    "    df['operatingSystem'] = df['device'].progress_apply(lambda x: json.loads(x)['operatingSystem'])\n",
    "    df['isMobile'] = df['device'].progress_apply(lambda x: json.loads(x)['isMobile'])\n",
    "    df['deviceCategory'] = df['device'].progress_apply(lambda x: json.loads(x)['deviceCategory'])\n",
    "\n",
    "    df = label_transform(df, device_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_device(train)\n",
    "test = process_device(test)\n",
    "cate_features += device_features\n",
    "print('process_device done')\n",
    "\n",
    "\n",
    "###############geoNetwork#####################\n",
    "geo_features = ['continent','subContinent','country','region','metro','city','networkDomain']\n",
    "\n",
    "def process_geo(df):\n",
    "    df['continent'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['continent'])\n",
    "    df['subContinent'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['subContinent'])\n",
    "    df['country'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['country'])\n",
    "    df['region'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['region'])\n",
    "    df['metro'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['metro'])\n",
    "    df['city'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['city'])\n",
    "    df['networkDomain'] = df['geoNetwork'].progress_apply(lambda x: json.loads(x)['networkDomain'])\n",
    "\n",
    "    df = label_transform(df, geo_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_geo(train)\n",
    "test = process_geo(test)\n",
    "cate_features += geo_features\n",
    "print('process_geo done')\n",
    "\n",
    "\n",
    "################totals####################\n",
    "view_features = ['hits','pageviews','newVisits','bounces','visitNumber']\n",
    "\n",
    "def process_totals(df):\n",
    "    df['hits'] = df['totals'].progress_apply(lambda x: json.loads(x)['hits']).astype(int)\n",
    "    df['pageviews'] = df['totals'].progress_apply(lambda x: json.loads(x)['pageviews'] if x.find('pageviews')>=0 else 0).astype(int)\n",
    "    df['bounces'] = df['totals'].progress_apply(lambda x: json.loads(x)['bounces'] if x.find('bounces')>=0 else 0).astype(int)\n",
    "    df['newVisits'] = df['totals'].progress_apply(lambda x: json.loads(x)['newVisits'] if x.find('newVisits')>=0 else 0).astype(int)\n",
    "    df['transactionRevenue'] = df['totals'].progress_apply(lambda x: json.loads(x)['transactionRevenue'] if x.find('transactionRevenue')>=0 else 0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_totals(train)\n",
    "test = process_totals(test)\n",
    "numeric_features += view_features\n",
    "print('process_totals done')\n",
    "\n",
    "################totals####################\n",
    "last_time_features = ['last_seconds','last_minutes']\n",
    "\n",
    "def process_last_time(df):\n",
    "    df['last_seconds'] = df['visitStartTime']-df['visitId']\n",
    "    df['last_minutes'] = (df['visitStartTime']-df['visitId'])/60\n",
    "    df['last_minutes'] = df['last_minutes'].astype(np.int64)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_last_time(train)\n",
    "test = process_last_time(test)\n",
    "numeric_features += last_time_features\n",
    "print('process_last_time done')\n",
    "\n",
    "################Multip transaction####################\n",
    "def process_transaction_count(df):\n",
    "    transaction_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "        \n",
    "    transac_count_df = transaction_df.count()\n",
    "    transac_count_df['fullVisitorId'] = transac_count_df.index\n",
    "    transac_count_df.rename(columns={\n",
    "        'transactionRevenue':'transaction_count'\n",
    "    }, inplace=True)\n",
    "    transac_count_df['transaction_count']=transac_count_df['transaction_count'].astype(int)\n",
    "\n",
    "    df = pd.merge(df, transac_count_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_transaction_count(train)\n",
    "test = process_transaction_count(test)\n",
    "numeric_features.append('transaction_count')\n",
    "print('process_transaction_count done')\n",
    "\n",
    "def process_buy_times(df):\n",
    "    transaction_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "\n",
    "    def count_buy_times(x):\n",
    "        buy_times = sum(x['transactionRevenue']>0)\n",
    "\n",
    "        return buy_times\n",
    "\n",
    "    bar().pandas(\"process_buy_times\")\n",
    "    buy_times_group = transaction_df.progress_apply(count_buy_times)\n",
    "    buy_times_df = pd.DataFrame({\n",
    "        'fullVisitorId':buy_times_group.index,\n",
    "        'buy_times':buy_times_group.values\n",
    "    })\n",
    "\n",
    "    df = pd.merge(df, buy_times_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_buy_times(train)\n",
    "test = process_buy_times(test)\n",
    "numeric_features.append('buy_times')\n",
    "print('process_buy_times done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89867bfc92e44ea383466595579ada58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49aed553e1a4c98b1e7eaa55f57fe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9a5f67ec174d7d84cf5c22de38882f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb8114c0492426b81c1b945e3dde729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dbc6b960dd4ba78e2a4eadb9409182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbda6c315b34a8eaf340eb8c0014df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e0854d41e495ba3de081689a1a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1da933549e47f186b10b2b092ccfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8232262bf2432ca28f8f6c766ec0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=903653), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6350c70e77411bba867896433b4351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c816e5a78b24643bcb61fceff2a4bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1112d615b85c44b7b002ad6b80a7c7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb308466406c4f1a90ea60f9a6c4d777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4df54f12beb4a16a5a93e1b754b8047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ec86c3cf0943508558de159fce79b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d387235e6343e6a0728df1feb51341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5749f6457c05469fa905ec9aaf391816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ca5d5902934837ae29a5c92ba366c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=804684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "process_traffic done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45934cdc48324e2888fceba8fccf8bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=714167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:70: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c885e5fabd4dafb41793136b4b7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=617242), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:71: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_revenue done\n",
      "train.shape (903653, 45)\n",
      "test.shape (804684, 51)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_adwordsClickInfo_field(x, field):\n",
    "    jo = json.loads(x)\n",
    "    \n",
    "    if x.find('adwordsClickInfo')>=0:\n",
    "        adwordsClickInfo = jo['adwordsClickInfo']\n",
    "        \n",
    "        if str(adwordsClickInfo).find(field)>=0:\n",
    "            return adwordsClickInfo[field]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def parse_adwordsClickInfo_page(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'page')\n",
    "\n",
    "def parse_adwordsClickInfo_slot(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'slot')\n",
    "\n",
    "def parse_adwordsClickInfo_gclId(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'gclId')\n",
    "\n",
    "def parse_adwordsClickInfo_adNetworkType(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'adNetworkType')\n",
    "\n",
    "def parse_adwordsClickInfo_isVideoAd(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'isVideoAd')\n",
    "\n",
    "traffic_features = ['campaign','source','medium','keyword','adwordsClickInfo_gclId_prefix','adwordsClickInfo_slot',\n",
    "                    'adwordsClickInfo_gclId','adwordsClickInfo_adNetworkType']\n",
    "\n",
    "def process_traffic(df):\n",
    "    df['campaign'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['campaign']).astype(str)\n",
    "    # need to merge nearly same record\n",
    "    df['source'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['source']).astype(str)\n",
    "    df['medium'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['medium']).astype(str)\n",
    "    # need to merge some keywords\n",
    "    df['keyword'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['keyword'] if x.find('keyword')>=0 else 0).astype(str)\n",
    "\n",
    "    df['adwordsClickInfo_page'] = df['trafficSource'].progress_apply(parse_adwordsClickInfo_page).astype(int)\n",
    "    df['adwordsClickInfo_slot'] = df['trafficSource'].progress_apply(parse_adwordsClickInfo_slot).astype(str)\n",
    "    df['adwordsClickInfo_gclId'] = df['trafficSource'].progress_apply(parse_adwordsClickInfo_gclId).astype(str)\n",
    "    df['adwordsClickInfo_gclId_prefix'] = df['adwordsClickInfo_gclId'].progress_apply(lambda x: x.split('_')[0] if type(x)!=int and x.find('_')>=0 else 0).astype(str)\n",
    "    df['adwordsClickInfo_adNetworkType'] = df['trafficSource'].progress_apply(parse_adwordsClickInfo_adNetworkType).astype(str)\n",
    "\n",
    "    df = label_transform(df, traffic_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_traffic(train)\n",
    "test = process_traffic(test)\n",
    "\n",
    "cate_features += traffic_features\n",
    "numeric_features.append('adwordsClickInfo_page')\n",
    "print('process_traffic done')\n",
    "\n",
    "###################################################### \n",
    "target = 'revenue'\n",
    "\n",
    "def process_revenue(df):\n",
    "    revenue_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId').agg('sum')\n",
    "    revenue_df['fullVisitorId'] = revenue_df.index\n",
    "    revenue_df[target] = revenue_df['transactionRevenue'].progress_apply(lambda x: np.log(x+1))\n",
    "\n",
    "    revenue_df.drop('transactionRevenue', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df, revenue_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = process_revenue(train)\n",
    "test = process_revenue(test)\n",
    "print('process_revenue done')\n",
    "\n",
    "removed_columns = ['device','geoNetwork','socialEngagementType','totals','trafficSource','transactionRevenue']\n",
    "train.drop(removed_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "unique fullVisitorId size  (903653, 45) 714167\n",
      "revisted_df.shape (282978, 45) 93492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93492/93492 [03:45<00:00, 414.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_merged_df.shape (93492, 45)\n",
      "df.shape (714167, 45)\n",
      "\n",
      "unique fullVisitorId size  (804684, 51) 617242\n",
      "revisted_df.shape (278860, 51) 91418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91418/91418 [04:18<00:00, 352.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_merged_df.shape (91418, 51)\n",
      "df.shape (617242, 51)\n",
      "merge_duplicated_into_new_row done\n",
      "\n",
      "train.shape (714167, 45)\n",
      "test.shape (617242, 51)\n"
     ]
    }
   ],
   "source": [
    "# Parallet groupby works well\n",
    "# Merge the same fullVisitorId records into one record\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "\n",
    "def applyParallel(dfGrouped, func, total):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = list(tqdm.tqdm(\n",
    "            p.imap(func, [group for name, group in dfGrouped]),\n",
    "            total=total)) \n",
    "\n",
    "    df = pd.concat(ret_list, axis=1)\n",
    "    df = df.T\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_group_func(df):\n",
    "    def merge_one_column(x):\n",
    "        new_merged_value=x.mode().values[0]\n",
    "        return new_merged_value\n",
    "\n",
    "    temp = df.apply(merge_one_column)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def merge_duplicated_into_new_row(df):\n",
    "    print()\n",
    "    print('unique fullVisitorId size ',df.shape, len(df['fullVisitorId'].unique()))\n",
    "\n",
    "    d_rows = df[df['fullVisitorId'].duplicated(keep=False)]\n",
    "    revisted_df = df.loc[d_rows.index]\n",
    "    unique_user_size = len(revisted_df['fullVisitorId'].unique())\n",
    "    \n",
    "    print('revisted_df.shape',revisted_df.shape, unique_user_size)\n",
    "\n",
    "    new_merged_df = applyParallel(revisted_df.groupby('fullVisitorId'), merge_group_func, unique_user_size)\n",
    "    print('new_merged_df.shape',new_merged_df.shape)\n",
    "\n",
    "    df.drop(d_rows.index, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df,new_merged_df], axis=0)\n",
    "    del d_rows\n",
    "    del new_merged_df\n",
    "    \n",
    "    print('df.shape',df.shape)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "train = merge_duplicated_into_new_row(train)\n",
    "test = merge_duplicated_into_new_row(test)\n",
    "print('merge_duplicated_into_new_row done')\n",
    "\n",
    "print()\n",
    "print('train.shape', train.shape)\n",
    "print('test.shape', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 10)       1000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          1100        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           dense_1[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          20100       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            101         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,051,401\n",
      "Trainable params: 1,051,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 571333 samples, validate on 142834 samples\n",
      "Epoch 1/5\n",
      "571333/571333 [==============================] - 46s 80us/step - loss: 0.4820 - mean_squared_error: 0.4820 - val_loss: 6.0437 - val_mean_squared_error: 6.0437\n",
      "Epoch 2/5\n",
      "571333/571333 [==============================] - 45s 79us/step - loss: 0.0615 - mean_squared_error: 0.0615 - val_loss: 6.2566 - val_mean_squared_error: 6.2566\n",
      "Epoch 3/5\n",
      "571333/571333 [==============================] - 46s 80us/step - loss: 0.0508 - mean_squared_error: 0.0508 - val_loss: 6.8659 - val_mean_squared_error: 6.8659\n",
      "Epoch 4/5\n",
      "571333/571333 [==============================] - 45s 80us/step - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 5.8204 - val_mean_squared_error: 5.8204\n",
      "Epoch 5/5\n",
      "571333/571333 [==============================] - 45s 79us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 6.1940 - val_mean_squared_error: 6.1940\n",
      "model.fit done\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Dropout, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "def get_model():\n",
    "    input_cate = Input((len(cate_features),))\n",
    "    input_numeric = Input((len(numeric_features),))\n",
    "    \n",
    "    x_cate = Embedding(100000, 10)(input_cate)\n",
    "    x_cate = Flatten()(x_cate)\n",
    "    x_cate = Dropout(0.2)(x_cate)\n",
    "    x_cate = Dense(100, activation='relu')(x_cate)\n",
    "    \n",
    "    \n",
    "    x_numeric = Dense(100, activation='relu')(input_numeric)\n",
    "    x_numeric = Dropout(0.2)(x_numeric)\n",
    "    \n",
    "    x = concatenate([x_cate,x_numeric])\n",
    "    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    output = Dense(1, kernel_initializer='normal')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_cate,input_numeric], outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.fit([train[cate_features], train[numeric_features]], train[target], validation_split=0.2, epochs=3, batch_size=100)\n",
    "print('model.fit done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape (617242, 51)\n",
      "(617242, 52)\n",
      "predict done\n"
     ]
    }
   ],
   "source": [
    "print('test.shape',test.shape)\n",
    "predict_test = model.predict([test[cate_features], test[numeric_features]],verbose=1)\n",
    "\n",
    "test['PredictedLogRevenue'] = predict_test\n",
    "\n",
    "test[['fullVisitorId','PredictedLogRevenue']].to_csv('GStore_keras_baseline.csv', index=False)\n",
    "print(test.shape)\n",
    "\n",
    "print('predict done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
