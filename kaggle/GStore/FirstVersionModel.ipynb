{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_date done\n",
      "process_device done\n",
      "process_geo done\n",
      "process_totals done\n",
      "process_last_time done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:148: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:149: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_transaction_count done\n",
      "process_buy_times done\n",
      "process_traffic done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:248: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:249: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_revenue done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/GStore/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "train = pd.read_csv(dir + train_file, low_memory=False)\n",
    "test = pd.read_csv(dir + test_file, low_memory=False)\n",
    "\n",
    "\n",
    "cate_features = []\n",
    "numeric_features = []\n",
    "\n",
    "train['fullVisitorId'] = train['fullVisitorId'].astype(str)\n",
    "test['fullVisitorId'] = test['fullVisitorId'].astype(str)\n",
    "\n",
    "train['channelGrouping'] = label.fit_transform(train['channelGrouping'])\n",
    "test['channelGrouping'] = label.fit_transform(test['channelGrouping'])\n",
    "\n",
    "cate_features.append('channelGrouping')\n",
    "\n",
    "def label_transform(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = label.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "####################################\n",
    "date_features = ['date','year','month','day','week','weekofyear','dayofweek','quarter','month_start','month_end']\n",
    "\n",
    "def process_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d',errors='ignore')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month_start'] = df['date'].dt.is_month_start\n",
    "    df['month_end'] = df['date'].dt.is_month_end\n",
    "\n",
    "    df = label_transform(df, date_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_date(train)\n",
    "test = process_date(test)\n",
    "cate_features += date_features\n",
    "print('process_date done')\n",
    "\n",
    "################device####################\n",
    "device_features = ['browser','operatingSystem','isMobile','deviceCategory']\n",
    "\n",
    "def process_device(df):\n",
    "    df['browser'] = df['device'].apply(lambda x: json.loads(x)['browser'])\n",
    "    df['operatingSystem'] = df['device'].apply(lambda x: json.loads(x)['operatingSystem'])\n",
    "    df['isMobile'] = df['device'].apply(lambda x: json.loads(x)['isMobile'])\n",
    "    df['deviceCategory'] = df['device'].apply(lambda x: json.loads(x)['deviceCategory'])\n",
    "\n",
    "    df = label_transform(df, device_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_device(train)\n",
    "test = process_device(test)\n",
    "cate_features += device_features\n",
    "print('process_device done')\n",
    "\n",
    "\n",
    "###############geoNetwork#####################\n",
    "geo_features = ['continent','subContinent','country','region','metro','city','networkDomain']\n",
    "\n",
    "def process_geo(df):\n",
    "    df['continent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['continent'])\n",
    "    df['subContinent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['subContinent'])\n",
    "    df['country'] = df['geoNetwork'].apply(lambda x: json.loads(x)['country'])\n",
    "    df['region'] = df['geoNetwork'].apply(lambda x: json.loads(x)['region'])\n",
    "    df['metro'] = df['geoNetwork'].apply(lambda x: json.loads(x)['metro'])\n",
    "    df['city'] = df['geoNetwork'].apply(lambda x: json.loads(x)['city'])\n",
    "    df['networkDomain'] = df['geoNetwork'].apply(lambda x: json.loads(x)['networkDomain'])\n",
    "\n",
    "    df = label_transform(df, geo_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_geo(train)\n",
    "test = process_geo(test)\n",
    "cate_features += geo_features\n",
    "print('process_geo done')\n",
    "\n",
    "\n",
    "################totals####################\n",
    "view_features = ['hits','pageviews','newVisits','bounces','visitNumber']\n",
    "\n",
    "def process_totals(df):\n",
    "    df['hits'] = df['totals'].apply(lambda x: json.loads(x)['hits']).astype(int)\n",
    "    df['pageviews'] = df['totals'].apply(lambda x: json.loads(x)['pageviews'] if x.find('pageviews')>=0 else 0).astype(int)\n",
    "    df['bounces'] = df['totals'].apply(lambda x: json.loads(x)['bounces'] if x.find('bounces')>=0 else 0).astype(int)\n",
    "    df['newVisits'] = df['totals'].apply(lambda x: json.loads(x)['newVisits'] if x.find('newVisits')>=0 else 0).astype(int)\n",
    "    df['transactionRevenue'] = df['totals'].apply(lambda x: json.loads(x)['transactionRevenue'] if x.find('transactionRevenue')>=0 else 0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_totals(train)\n",
    "test = process_totals(test)\n",
    "numeric_features += view_features\n",
    "print('process_totals done')\n",
    "\n",
    "\n",
    "################totals####################\n",
    "last_time_features = ['last_seconds','last_minutes']\n",
    "\n",
    "def process_last_time(df):\n",
    "    df['last_seconds'] = df['visitStartTime']-df['visitId']\n",
    "    df['last_minutes'] = (df['visitStartTime']-df['visitId'])/60\n",
    "    df['last_minutes'] = df['last_minutes'].astype(np.int64)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_last_time(train)\n",
    "test = process_last_time(test)\n",
    "numeric_features += last_time_features\n",
    "print('process_last_time done')\n",
    "\n",
    "\n",
    "################Multip transaction####################\n",
    "def process_transaction_count(df):\n",
    "    transaction_df = train[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "        \n",
    "    transac_count_df = transaction_df.count()\n",
    "    transac_count_df['fullVisitorId'] = transac_count_df.index\n",
    "    transac_count_df.rename(columns={\n",
    "        'transactionRevenue':'transaction_count'\n",
    "    }, inplace=True)\n",
    "    transac_count_df['transaction_count']=transac_count_df['transaction_count'].astype(int)\n",
    "\n",
    "    df = pd.merge(df, transac_count_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_transaction_count(train)\n",
    "test = process_transaction_count(test)\n",
    "numeric_features.append('transaction_count')\n",
    "print('process_transaction_count done')\n",
    "\n",
    "\n",
    "def process_buy_times(df):\n",
    "    transaction_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "\n",
    "    def count_buy_times(x):\n",
    "        buy_times = sum(x['transactionRevenue']>0)\n",
    "\n",
    "        return buy_times\n",
    "\n",
    "    buy_times_group = transaction_df.apply(count_buy_times)\n",
    "    buy_times_df = pd.DataFrame({\n",
    "        'fullVisitorId':buy_times_group.index,\n",
    "        'buy_times':buy_times_group.values\n",
    "    })\n",
    "\n",
    "    df = pd.merge(df, buy_times_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_buy_times(train)\n",
    "test = process_buy_times(test)\n",
    "numeric_features.append('buy_times')\n",
    "print('process_buy_times done')\n",
    "\n",
    "\n",
    "def parse_adwordsClickInfo_field(x, field):\n",
    "    jo = json.loads(x)\n",
    "    \n",
    "    if x.find('adwordsClickInfo')>=0:\n",
    "        adwordsClickInfo = jo['adwordsClickInfo']\n",
    "        \n",
    "        if str(adwordsClickInfo).find(field)>=0:\n",
    "            return adwordsClickInfo[field]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def parse_adwordsClickInfo_page(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'page')\n",
    "\n",
    "def parse_adwordsClickInfo_slot(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'slot')\n",
    "\n",
    "def parse_adwordsClickInfo_gclId(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'gclId')\n",
    "\n",
    "def parse_adwordsClickInfo_adNetworkType(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'adNetworkType')\n",
    "\n",
    "def parse_adwordsClickInfo_isVideoAd(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'isVideoAd')\n",
    "\n",
    "traffic_features = ['campaign','source','medium','keyword','adwordsClickInfo_gclId_prefix','adwordsClickInfo_slot',\n",
    "                    'adwordsClickInfo_gclId','adwordsClickInfo_adNetworkType']\n",
    "\n",
    "def process_traffic(df):\n",
    "    df['campaign'] = df['trafficSource'].apply(lambda x: json.loads(x)['campaign']).astype(str)\n",
    "    # need to merge nearly same record\n",
    "    df['source'] = df['trafficSource'].apply(lambda x: json.loads(x)['source']).astype(str)\n",
    "    df['medium'] = df['trafficSource'].apply(lambda x: json.loads(x)['medium']).astype(str)\n",
    "    # need to merge some keywords\n",
    "    df['keyword'] = df['trafficSource'].apply(lambda x: json.loads(x)['keyword'] if x.find('keyword')>=0 else 0).astype(str)\n",
    "\n",
    "    df['adwordsClickInfo_page'] = df['trafficSource'].apply(parse_adwordsClickInfo_page).astype(int)\n",
    "    df['adwordsClickInfo_slot'] = df['trafficSource'].apply(parse_adwordsClickInfo_slot).astype(str)\n",
    "    df['adwordsClickInfo_gclId'] = df['trafficSource'].apply(parse_adwordsClickInfo_gclId).astype(str)\n",
    "    df['adwordsClickInfo_gclId_prefix'] = df['adwordsClickInfo_gclId'].apply(lambda x: x.split('_')[0] if type(x)!=int and x.find('_')>=0 else 0).astype(str)\n",
    "    df['adwordsClickInfo_adNetworkType'] = df['trafficSource'].apply(parse_adwordsClickInfo_adNetworkType).astype(str)\n",
    "\n",
    "    df = label_transform(df, traffic_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_traffic(train)\n",
    "test = process_traffic(test)\n",
    "\n",
    "cate_features += traffic_features\n",
    "numeric_features.append('adwordsClickInfo_page')\n",
    "print('process_traffic done')\n",
    "\n",
    "\n",
    "###################################################### \n",
    "target = 'revenue'\n",
    "\n",
    "def process_revenue(df):\n",
    "    revenue_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId').agg('sum')\n",
    "    revenue_df['fullVisitorId'] = revenue_df.index\n",
    "    revenue_df[target] = revenue_df['transactionRevenue'].apply(lambda x: np.log(x+1))\n",
    "\n",
    "    revenue_df.drop('transactionRevenue', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df, revenue_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = process_revenue(train)\n",
    "test = process_revenue(test)\n",
    "print('process_revenue done')\n",
    "\n",
    "removed_columns = ['device','geoNetwork','socialEngagementType','totals','trafficSource','transactionRevenue']\n",
    "train.drop(removed_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train[target].values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique fullVisitorId size  714167\n",
      "revisted_df.shape (282978, 45)\n",
      "new_merged_df.shape (93492, 45)\n",
      "df.shape (714167, 45)\n",
      "unique fullVisitorId size  7679\n",
      "revisted_df.shape (15844, 51)\n",
      "new_merged_df.shape (3239, 51)\n",
      "df.shape (7679, 51)\n",
      "merge_duplicated_into_new_row done\n",
      "train.shape (714167, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelGrouping</th>\n",
       "      <th>date</th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>source</th>\n",
       "      <th>medium</th>\n",
       "      <th>keyword</th>\n",
       "      <th>adwordsClickInfo_page</th>\n",
       "      <th>adwordsClickInfo_slot</th>\n",
       "      <th>adwordsClickInfo_gclId</th>\n",
       "      <th>adwordsClickInfo_gclId_prefix</th>\n",
       "      <th>adwordsClickInfo_adNetworkType</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1131660440785968503</td>\n",
       "      <td>1131660440785968503_1472830385</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>1</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>377306020877927890</td>\n",
       "      <td>377306020877927890_1472880147</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>1</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>3895546263509774583</td>\n",
       "      <td>3895546263509774583_1472865386</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>1</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4763447161404445595</td>\n",
       "      <td>4763447161404445595_1472881213</td>\n",
       "      <td>1472881213</td>\n",
       "      <td>1</td>\n",
       "      <td>1472881213</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>1099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2938943183656635653</td>\n",
       "      <td>2938943183656635653_1472807194</td>\n",
       "      <td>1472807194</td>\n",
       "      <td>1</td>\n",
       "      <td>1472807194</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1905672039242460897</td>\n",
       "      <td>1905672039242460897_1472817241</td>\n",
       "      <td>1472817241</td>\n",
       "      <td>1</td>\n",
       "      <td>1472817241</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>537222803633850821</td>\n",
       "      <td>537222803633850821_1472812602</td>\n",
       "      <td>1472812602</td>\n",
       "      <td>1</td>\n",
       "      <td>1472812602</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4445454811831400414</td>\n",
       "      <td>4445454811831400414_1472805784</td>\n",
       "      <td>1472805784</td>\n",
       "      <td>1</td>\n",
       "      <td>1472805784</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>9499785259412240342</td>\n",
       "      <td>9499785259412240342_1472812272</td>\n",
       "      <td>1472812272</td>\n",
       "      <td>1</td>\n",
       "      <td>1472812272</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0523069750702990437</td>\n",
       "      <td>0523069750702990437_1472834967</td>\n",
       "      <td>1472834967</td>\n",
       "      <td>1</td>\n",
       "      <td>1472834967</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   channelGrouping date        fullVisitorId                       sessionId  \\\n",
       "0                4   32  1131660440785968503  1131660440785968503_1472830385   \n",
       "1                4   32   377306020877927890   377306020877927890_1472880147   \n",
       "2                4   32  3895546263509774583  3895546263509774583_1472865386   \n",
       "3                4   32  4763447161404445595  4763447161404445595_1472881213   \n",
       "6                4   32  2938943183656635653  2938943183656635653_1472807194   \n",
       "7                4   32  1905672039242460897  1905672039242460897_1472817241   \n",
       "8                4   32   537222803633850821   537222803633850821_1472812602   \n",
       "9                4   32  4445454811831400414  4445454811831400414_1472805784   \n",
       "10               4   32  9499785259412240342  9499785259412240342_1472812272   \n",
       "11               4   32  0523069750702990437  0523069750702990437_1472834967   \n",
       "\n",
       "       visitId visitNumber visitStartTime year month day   ...   campaign  \\\n",
       "0   1472830385           1     1472830385    0     8   1   ...          0   \n",
       "1   1472880147           1     1472880147    0     8   1   ...          0   \n",
       "2   1472865386           1     1472865386    0     8   1   ...          0   \n",
       "3   1472881213           1     1472881213    0     8   1   ...          0   \n",
       "6   1472807194           1     1472807194    0     8   1   ...          0   \n",
       "7   1472817241           1     1472817241    0     8   1   ...          0   \n",
       "8   1472812602           1     1472812602    0     8   1   ...          0   \n",
       "9   1472805784           1     1472805784    0     8   1   ...          0   \n",
       "10  1472812272           1     1472812272    0     8   1   ...          0   \n",
       "11  1472834967           1     1472834967    0     8   1   ...          0   \n",
       "\n",
       "   source medium keyword adwordsClickInfo_page adwordsClickInfo_slot  \\\n",
       "0     149      5      11                     0                     0   \n",
       "1     149      5      11                     0                     0   \n",
       "2     149      5      11                     0                     0   \n",
       "3     149      5    1099                     0                     0   \n",
       "6     149      5      11                     0                     0   \n",
       "7     149      5      11                     0                     0   \n",
       "8     149      5      11                     0                     0   \n",
       "9     149      5      11                     0                     0   \n",
       "10    149      5      11                     0                     0   \n",
       "11    149      5      11                     0                     0   \n",
       "\n",
       "   adwordsClickInfo_gclId adwordsClickInfo_gclId_prefix  \\\n",
       "0                       0                             0   \n",
       "1                       0                             0   \n",
       "2                       0                             0   \n",
       "3                       0                             0   \n",
       "6                       0                             0   \n",
       "7                       0                             0   \n",
       "8                       0                             0   \n",
       "9                       0                             0   \n",
       "10                      0                             0   \n",
       "11                      0                             0   \n",
       "\n",
       "   adwordsClickInfo_adNetworkType revenue  \n",
       "0                               0       0  \n",
       "1                               0       0  \n",
       "2                               0       0  \n",
       "3                               0       0  \n",
       "6                               0       0  \n",
       "7                               0       0  \n",
       "8                               0       0  \n",
       "9                               0       0  \n",
       "10                              0       0  \n",
       "11                              0       0  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallet groupby works well\n",
    "# Merge the same fullVisitorId records into one record\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "\n",
    "    df = pd.concat(ret_list, axis=1)\n",
    "    df = df.T\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_group_func(df):\n",
    "    def merge_one_column(x):\n",
    "        new_merged_value=x.mode().values[0]\n",
    "        return new_merged_value\n",
    "\n",
    "    temp = df.apply(merge_one_column)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def merge_duplicated_into_new_row(df):\n",
    "    print('unique fullVisitorId size ',len(df['fullVisitorId'].unique()))\n",
    "\n",
    "    d_rows = df[df['fullVisitorId'].duplicated(keep=False)]\n",
    "    revisted_df = df.loc[d_rows.index]\n",
    "    print('revisted_df.shape',revisted_df.shape)\n",
    "\n",
    "    new_merged_df = applyParallel(revisted_df.groupby('fullVisitorId'),merge_group_func)\n",
    "    print('new_merged_df.shape',new_merged_df.shape)\n",
    "\n",
    "    df.drop(d_rows.index, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df,new_merged_df], axis=0)\n",
    "    del d_rows\n",
    "    del new_merged_df\n",
    "    \n",
    "    print('df.shape',df.shape)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "train = merge_duplicated_into_new_row(train)\n",
    "test = merge_duplicated_into_new_row(test)\n",
    "print('merge_duplicated_into_new_row done')\n",
    "\n",
    "print('train.shape', train.shape)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 10)       1000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 300)          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 300)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          1100        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          30100       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200)          0           dense_5[0][0]                    \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          20100       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            101         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,051,401\n",
      "Trainable params: 1,051,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 571333 samples, validate on 142834 samples\n",
      "Epoch 1/5\n",
      "571333/571333 [==============================] - 67s 117us/step - loss: 0.4260 - mean_squared_error: 0.4260 - val_loss: 5.9881 - val_mean_squared_error: 5.9881\n",
      "Epoch 2/5\n",
      "571333/571333 [==============================] - 65s 113us/step - loss: 0.0740 - mean_squared_error: 0.0740 - val_loss: 6.0269 - val_mean_squared_error: 6.0269\n",
      "Epoch 3/5\n",
      "571333/571333 [==============================] - 64s 111us/step - loss: 0.0524 - mean_squared_error: 0.0524 - val_loss: 5.9301 - val_mean_squared_error: 5.9301\n",
      "Epoch 4/5\n",
      "571333/571333 [==============================] - 66s 116us/step - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 5.5084 - val_mean_squared_error: 5.5084\n",
      "Epoch 5/5\n",
      "571333/571333 [==============================] - 67s 117us/step - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 5.2581 - val_mean_squared_error: 5.2581\n",
      "model.fit done\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Dropout, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "def get_model():\n",
    "    input_cate = Input((len(cate_features),))\n",
    "    input_numeric = Input((len(numeric_features),))\n",
    "    \n",
    "    x_cate = Embedding(100000, 10)(input_cate)\n",
    "    x_cate = Flatten()(x_cate)\n",
    "    x_cate = Dropout(0.2)(x_cate)\n",
    "    x_cate = Dense(100, activation='relu')(x_cate)\n",
    "    \n",
    "    \n",
    "    x_numeric = Dense(100, activation='relu')(input_numeric)\n",
    "    x_numeric = Dropout(0.2)(x_numeric)\n",
    "    \n",
    "    x = concatenate([x_cate,x_numeric])\n",
    "    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    output = Dense(1, kernel_initializer='normal')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_cate,input_numeric], outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.fit([train[cate_features], train[numeric_features]], train[target], validation_split=0.2, epochs=5, batch_size=100)\n",
    "print('model.fit done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict done\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict([test[cate_features], test[numeric_features]])\n",
    "\n",
    "test['PredictedLogRevenue'] = predict_test\n",
    "\n",
    "\n",
    "print('predict done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7679, 2)\n"
     ]
    }
   ],
   "source": [
    "target_sum = test[['fullVisitorId','PredictedLogRevenue']].groupby('fullVisitorId', as_index=False).agg('sum')\n",
    "\n",
    "\n",
    "target_sum.to_csv('GStore_keras_baseline.csv', index=False)\n",
    "print(target_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
