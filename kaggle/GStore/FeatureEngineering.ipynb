{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:147: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:148: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:244: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:245: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/GStore/'\n",
    "train_file = 'train_small.csv'\n",
    "test_file = 'test_small.csv'\n",
    "\n",
    "train = pd.read_csv(dir + train_file, low_memory=False)\n",
    "test = pd.read_csv(dir + test_file, low_memory=False)\n",
    "\n",
    "\n",
    "cate_features = []\n",
    "numeric_features = []\n",
    "\n",
    "train['fullVisitorId'] = train['fullVisitorId'].astype(str)\n",
    "test['fullVisitorId'] = test['fullVisitorId'].astype(str)\n",
    "\n",
    "train['channelGrouping'] = label.fit_transform(train['channelGrouping'])\n",
    "test['channelGrouping'] = label.fit_transform(test['channelGrouping'])\n",
    "\n",
    "cate_features.append('channelGrouping')\n",
    "\n",
    "def label_transform(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = label.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "####################################\n",
    "date_features = ['date','year','month','day','week','weekofyear','dayofweek','quarter','month_start','month_end']\n",
    "\n",
    "def process_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d',errors='ignore')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month_start'] = df['date'].dt.is_month_start\n",
    "    df['month_end'] = df['date'].dt.is_month_end\n",
    "\n",
    "    df = label_transform(df, date_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_date(train)\n",
    "test = process_date(test)\n",
    "cate_features += date_features\n",
    "\n",
    "\n",
    "################device####################\n",
    "device_features = ['browser','operatingSystem','isMobile','deviceCategory']\n",
    "\n",
    "def process_device(df):\n",
    "    df['browser'] = df['device'].apply(lambda x: json.loads(x)['browser'])\n",
    "    df['operatingSystem'] = df['device'].apply(lambda x: json.loads(x)['operatingSystem'])\n",
    "    df['isMobile'] = df['device'].apply(lambda x: json.loads(x)['isMobile'])\n",
    "    df['deviceCategory'] = df['device'].apply(lambda x: json.loads(x)['deviceCategory'])\n",
    "\n",
    "    df = label_transform(df, device_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_device(train)\n",
    "test = process_device(test)\n",
    "cate_features += device_features\n",
    "\n",
    "\n",
    "\n",
    "###############geoNetwork#####################\n",
    "geo_features = ['continent','subContinent','country','region','metro','city','networkDomain']\n",
    "\n",
    "def process_geo(df):\n",
    "    df['continent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['continent'])\n",
    "    df['subContinent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['subContinent'])\n",
    "    df['country'] = df['geoNetwork'].apply(lambda x: json.loads(x)['country'])\n",
    "    df['region'] = df['geoNetwork'].apply(lambda x: json.loads(x)['region'])\n",
    "    df['metro'] = df['geoNetwork'].apply(lambda x: json.loads(x)['metro'])\n",
    "    df['city'] = df['geoNetwork'].apply(lambda x: json.loads(x)['city'])\n",
    "    df['networkDomain'] = df['geoNetwork'].apply(lambda x: json.loads(x)['networkDomain'])\n",
    "\n",
    "    df = label_transform(df, geo_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_geo(train)\n",
    "test = process_geo(test)\n",
    "cate_features += geo_features\n",
    "\n",
    "\n",
    "\n",
    "################totals####################\n",
    "view_features = ['hits','pageviews','newVisits','bounces','visitNumber']\n",
    "\n",
    "def process_totals(df):\n",
    "    df['hits'] = df['totals'].apply(lambda x: json.loads(x)['hits']).astype(int)\n",
    "    df['pageviews'] = df['totals'].apply(lambda x: json.loads(x)['pageviews'] if x.find('pageviews')>=0 else 0).astype(int)\n",
    "    df['bounces'] = df['totals'].apply(lambda x: json.loads(x)['bounces'] if x.find('bounces')>=0 else 0).astype(int)\n",
    "    df['newVisits'] = df['totals'].apply(lambda x: json.loads(x)['newVisits'] if x.find('newVisits')>=0 else 0).astype(int)\n",
    "    df['transactionRevenue'] = df['totals'].apply(lambda x: json.loads(x)['transactionRevenue'] if x.find('transactionRevenue')>=0 else 0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_totals(train)\n",
    "test = process_totals(test)\n",
    "numeric_features += view_features\n",
    "\n",
    "\n",
    "\n",
    "################totals####################\n",
    "last_time_features = ['last_seconds','last_minutes']\n",
    "\n",
    "def process_last_time(df):\n",
    "    df['last_seconds'] = df['visitStartTime']-df['visitId']\n",
    "    df['last_minutes'] = (df['visitStartTime']-df['visitId'])/60\n",
    "    df['last_minutes'] = df['last_minutes'].astype(np.int64)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_last_time(train)\n",
    "test = process_last_time(test)\n",
    "numeric_features += last_time_features\n",
    "\n",
    "\n",
    "################Multip transaction####################\n",
    "def process_transaction_count(df):\n",
    "    transaction_df = train[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "        \n",
    "    transac_count_df = transaction_df.count()\n",
    "    transac_count_df['fullVisitorId'] = transac_count_df.index\n",
    "    transac_count_df.rename(columns={\n",
    "        'transactionRevenue':'transaction_count'\n",
    "    }, inplace=True)\n",
    "    transac_count_df['transaction_count'] = transac_count_df['transaction_count'].astype(int)\n",
    "\n",
    "    df = pd.merge(df, transac_count_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_transaction_count(train)\n",
    "test = process_transaction_count(test)\n",
    "numeric_features.append('transaction_count')\n",
    "\n",
    "def process_buy_times(df):\n",
    "    transaction_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId')\n",
    "\n",
    "    def count_buy_times(x):\n",
    "        buy_times = sum(x['transactionRevenue']>0)\n",
    "\n",
    "        return buy_times\n",
    "\n",
    "    buy_times_group = transaction_df.apply(count_buy_times)\n",
    "    buy_times_df = pd.DataFrame({\n",
    "        'fullVisitorId':buy_times_group.index,\n",
    "        'buy_times':buy_times_group.values\n",
    "    })\n",
    "\n",
    "    df = pd.merge(df, buy_times_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_buy_times(train)\n",
    "test = process_buy_times(test)\n",
    "numeric_features.append('buy_times')\n",
    "\n",
    "\n",
    "def parse_adwordsClickInfo_field(x, field):\n",
    "    jo = json.loads(x)\n",
    "    \n",
    "    if x.find('adwordsClickInfo')>=0:\n",
    "        adwordsClickInfo = jo['adwordsClickInfo']\n",
    "        \n",
    "        if str(adwordsClickInfo).find(field)>=0:\n",
    "            return adwordsClickInfo[field]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def parse_adwordsClickInfo_page(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'page')\n",
    "\n",
    "def parse_adwordsClickInfo_slot(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'slot')\n",
    "\n",
    "def parse_adwordsClickInfo_gclId(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'gclId')\n",
    "\n",
    "def parse_adwordsClickInfo_adNetworkType(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'adNetworkType')\n",
    "\n",
    "def parse_adwordsClickInfo_isVideoAd(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'isVideoAd')\n",
    "\n",
    "traffic_features = ['campaign','source','medium','keyword','adwordsClickInfo_gclId_prefix','adwordsClickInfo_slot',\n",
    "                    'adwordsClickInfo_gclId','adwordsClickInfo_adNetworkType']\n",
    "\n",
    "def process_traffic(df):\n",
    "    df['campaign'] = df['trafficSource'].apply(lambda x: json.loads(x)['campaign']).astype(str)\n",
    "    # need to merge nearly same record\n",
    "    df['source'] = df['trafficSource'].apply(lambda x: json.loads(x)['source']).astype(str)\n",
    "    df['medium'] = df['trafficSource'].apply(lambda x: json.loads(x)['medium']).astype(str)\n",
    "    # need to merge some keywords\n",
    "    df['keyword'] = df['trafficSource'].apply(lambda x: json.loads(x)['keyword'] if x.find('keyword')>=0 else 0).astype(str)\n",
    "\n",
    "    df['adwordsClickInfo_page'] = df['trafficSource'].apply(parse_adwordsClickInfo_page).astype(int)\n",
    "    df['adwordsClickInfo_slot'] = df['trafficSource'].apply(parse_adwordsClickInfo_slot).astype(str)\n",
    "    df['adwordsClickInfo_gclId'] = df['trafficSource'].apply(parse_adwordsClickInfo_gclId).astype(str)\n",
    "    df['adwordsClickInfo_gclId_prefix'] = df['adwordsClickInfo_gclId'].apply(lambda x: x.split('_')[0] if type(x)!=int and x.find('_')>=0 else 0).astype(str)\n",
    "    df['adwordsClickInfo_adNetworkType'] = df['trafficSource'].apply(parse_adwordsClickInfo_adNetworkType).astype(str)\n",
    "\n",
    "    df = label_transform(df, traffic_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_traffic(train)\n",
    "test = process_traffic(test)\n",
    "\n",
    "cate_features += traffic_features\n",
    "numeric_features.append('adwordsClickInfo_page')\n",
    "\n",
    "\n",
    "\n",
    "###################################################### \n",
    "target = 'revenue'\n",
    "\n",
    "def process_revenue(df):\n",
    "    revenue_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId').agg('sum')\n",
    "    revenue_df['fullVisitorId'] = revenue_df.index\n",
    "    revenue_df[target] = revenue_df['transactionRevenue'].apply(lambda x: np.log(x+1))\n",
    "    \n",
    "    revenue_df.drop('transactionRevenue', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df, revenue_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = process_revenue(train)\n",
    "test = process_revenue(test)\n",
    "\n",
    "removed_columns = ['device','geoNetwork','socialEngagementType','totals','trafficSource']\n",
    "train.drop(removed_columns, axis=1, inplace=True)\n",
    "train.columns\n",
    "\n",
    "\n",
    "print(len(train['fullVisitorId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated id size 609\n",
      "channelGrouping\n",
      "date\n",
      "fullVisitorId\n",
      "sessionId\n",
      "visitId\n",
      "visitNumber\n",
      "visitStartTime\n",
      "year\n",
      "month\n",
      "day\n",
      "week\n",
      "weekofyear\n",
      "dayofweek\n",
      "quarter\n",
      "month_start\n",
      "month_end\n",
      "browser\n",
      "operatingSystem\n",
      "isMobile\n",
      "deviceCategory\n",
      "continent\n",
      "subContinent\n",
      "country\n",
      "region\n",
      "metro\n",
      "city\n",
      "networkDomain\n",
      "hits\n",
      "pageviews\n",
      "bounces\n",
      "newVisits\n",
      "transactionRevenue\n",
      "last_seconds\n",
      "last_minutes\n",
      "transaction_count\n",
      "buy_times\n",
      "campaign\n",
      "source\n",
      "medium\n",
      "keyword\n",
      "adwordsClickInfo_page\n",
      "adwordsClickInfo_slot\n",
      "adwordsClickInfo_gclId\n",
      "adwordsClickInfo_gclId_prefix\n",
      "adwordsClickInfo_adNetworkType\n",
      "revenue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "channelGrouping    None\n",
       "date               None\n",
       "fullVisitorId      None\n",
       "sessionId          None\n",
       "visitId            None\n",
       "dtype: object"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_rows = train[train['fullVisitorId'].duplicated(keep=False)]\n",
    "\n",
    "\n",
    "def merge_data(x):\n",
    "    mode_array = x.mode().values\n",
    "    \n",
    "    return mode_array[0]\n",
    "\n",
    "duplicated_ids = d_rows['fullVisitorId'].unique()\n",
    "print('duplicated id size',len(duplicated_ids))\n",
    "train_copy = train.copy()\n",
    "\n",
    "duplicated_df = pd.DataFrame({\n",
    "    'fullVisitorId':duplicated_ids\n",
    "})\n",
    "\n",
    "temp = train_copy.apply(lambda x: print(x.name), axis=0)\n",
    "\n",
    "temp.head()\n",
    "\n",
    "# for id in unique_ids:\n",
    "#     temp_mode = train[train['fullVisitorId'] == id].apply(merge_data, axis=0)\n",
    "    \n",
    "#     train = train.append(temp_mode,ignore_index=True)\n",
    "\n",
    "    \n",
    "# train.drop(d_rows.index, inplace=True)\n",
    "\n",
    "# print('train.shape',train.shape)\n",
    "# print('d_rows.shape',d_rows.shape)\n",
    "\n",
    "# print(len(d_rows['fullVisitorId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         17.97189053 17.08317659 17.44940573 19.54128281 18.03545936\n",
      " 17.37314175 20.16840125 15.94135859 19.79624272 16.99689222 17.38447974\n",
      " 18.21611359 20.11871544 18.58058627 17.59308749 19.64142217 17.35589003\n",
      " 17.38193988 19.64159916 17.33211782 20.26992265 18.45256694 19.53736214\n",
      " 15.45450747 18.32472036 16.17542081 17.06122308 19.28268423 16.94969962\n",
      " 20.20223835 17.98527177 17.61749548 17.2296242  16.21249642 17.03398634\n",
      " 16.6469577  16.01051055 17.2404245  19.27917286 16.29707839 17.86851217\n",
      " 19.00378982 18.20699236 19.37376618 18.48139949 17.46226511 17.7963131\n",
      " 17.2806213  18.3878476  19.39444762 18.47061312 20.79744524 15.75999126\n",
      " 19.63056604 19.51515114 17.117992   17.3412827  17.72753358 18.37954621\n",
      " 18.50823365 19.52045902 17.66926868 16.63629409 18.5474896  15.82774348\n",
      " 16.40252251 18.1974122  18.12751702 17.86868586 16.42484486 16.11609375\n",
      " 17.26518062 16.86098497 18.52648117 18.97387597 16.64813555 17.45204356\n",
      " 19.31284271 17.57601278 16.68284117 16.81074276 15.89370144 14.5036457\n",
      " 16.53614794 17.2790566  18.75464986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: 'fullVisitorId' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(dir + train_file, low_memory=False)\n",
    "test = pd.read_csv(dir + test_file, low_memory=False)\n",
    "\n",
    "\n",
    "def process_totals(df):\n",
    "    df['hits'] = df['totals'].apply(lambda x: json.loads(x)['hits']).astype(int)\n",
    "    df['pageviews'] = df['totals'].apply(lambda x: json.loads(x)['pageviews'] if x.find('pageviews')>=0 else 0).astype(int)\n",
    "    df['bounces'] = df['totals'].apply(lambda x: json.loads(x)['bounces'] if x.find('bounces')>=0 else 0).astype(int)\n",
    "    df['newVisits'] = df['totals'].apply(lambda x: json.loads(x)['newVisits'] if x.find('newVisits')>=0 else 0).astype(int)\n",
    "    df['transactionRevenue'] = df['totals'].apply(lambda x: json.loads(x)['transactionRevenue'] if x.find('transactionRevenue')>=0 else 0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_totals(train)\n",
    "\n",
    "\n",
    "target = 'revenue'\n",
    "\n",
    "def process_revenue(df):\n",
    "    revenue_df = df[['fullVisitorId','transactionRevenue']].groupby('fullVisitorId').agg('sum')\n",
    "    revenue_df['fullVisitorId'] = revenue_df.index\n",
    "    revenue_df[target] = revenue_df['transactionRevenue'].apply(lambda x: np.log(x+1))\n",
    "    \n",
    "    revenue_df.drop('transactionRevenue', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df, revenue_df, on='fullVisitorId')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = process_revenue(train)\n",
    "\n",
    "transactionRevenue = train[target].unique()\n",
    "\n",
    "print(transactionRevenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactionRevenue.mean 17.629718972025067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('transactionRevenue.mean',transactionRevenue.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby works well \n",
    "from tqdm import tqdm\n",
    "\n",
    "print('unique fullVisitorId size ',len(train['fullVisitorId'].unique()))\n",
    "\n",
    "d_rows = train[train['fullVisitorId'].duplicated(keep=False)]\n",
    "revisted_df = train.loc[d_rows.index]\n",
    "\n",
    "\n",
    "def merge_func(df):\n",
    "    def merge_one_column(x):\n",
    "        new_merged_value=x.mode().values[0]\n",
    "        return new_merged_value\n",
    "    \n",
    "    temp = df.apply(merge_one_column)\n",
    "    return temp\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "merged_df = revisted_df.loc[0:100].groupby('fullVisitorId').progress_apply(merge_func)\n",
    "merged_df['fullVisitorId'] = merged_df.index\n",
    "\n",
    "print('*'*80)\n",
    "print(merged_df.shape)\n",
    "merged_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another solution of groupby mode \n",
    "# works well\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_func(group):\n",
    "    new_value = group.mode().values[0]\n",
    "    print('',group,group.name,group.values,new_value)\n",
    "    print('-'*50)\n",
    "    \n",
    "    return new_value\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "merged_df = revisted_df.loc[0:10].groupby('fullVisitorId').agg(merge_func)\n",
    "merged_df['fullVisitorId'] = merged_df.index\n",
    "\n",
    "print(merged_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
