{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一条访问记录，作为一条训练数据。\n",
    "# Y = transactionRevenue/1000000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1cbb45bc98427c8af2e0fe95fd67d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train.shape (903653, 12)\n",
      "test.shape (804684, 12)\n",
      "\n",
      "process_datetime done\n",
      "process_device done\n",
      "process_geo done\n",
      "process_totals done\n",
      "process_last_time done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09e0053bbc343eeb76ee82581a9c468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=902755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4672acdce67a489fbe6a3c1666f8ff86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=902755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d72622901149faa135793c502bd621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76496f411ca14b36abe58caf6429ed3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "process_traffic done\n",
      "train.shape (902755, 45)\n",
      "test.shape (803863, 45)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as bar;\n",
    "bar().pandas(\"track progress\")\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/GStore/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "train = pd.read_csv(dir + train_file, low_memory=False)\n",
    "test = pd.read_csv(dir + test_file, low_memory=False)\n",
    "# train.set_index('sessionId')\n",
    "# test.set_index('sessionId')\n",
    "\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "print()\n",
    "\n",
    "train.drop_duplicates(subset='sessionId', keep='last', inplace=True)\n",
    "test.drop_duplicates(subset='sessionId', keep='last', inplace=True)\n",
    "\n",
    "cate_features = []\n",
    "basic_num_features = []\n",
    "advance_num_features = []\n",
    "\n",
    "desc_features = ['fullVisitorId','sessionId','visitId','visitStartTime']\n",
    "\n",
    "train['fullVisitorId'] = train['fullVisitorId'].astype(str)\n",
    "test['fullVisitorId'] = test['fullVisitorId'].astype(str)\n",
    "\n",
    "train['channelGrouping'] = label.fit_transform(train['channelGrouping'])\n",
    "test['channelGrouping'] = label.fit_transform(test['channelGrouping'])\n",
    "\n",
    "cate_features.append('channelGrouping')\n",
    "\n",
    "def label_transform(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = label.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "####################################\n",
    "date_cate_features = ['date','year']\n",
    "date_num_features = ['month','day','week','weekofyear','dayofweek','quarter','month_start','month_end']\n",
    "\n",
    "def process_datetime(df):\n",
    "    df['datetime'] = pd.to_datetime(df['date'], format='%Y%m%d',errors='ignore')\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['month'] =df['month'].astype(int)\n",
    "    \n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['day'] = df['day'].astype(int)\n",
    "\n",
    "    df['week'] = df['datetime'].dt.week\n",
    "    df['week'] = df['week'].astype(int)\n",
    "    \n",
    "    df['weekofyear'] = df['datetime'].dt.weekofyear\n",
    "    df['weekofyear'] = df['weekofyear'].astype(int)\n",
    "    \n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    df['dayofweek'] = df['dayofweek'].astype(int)\n",
    "    \n",
    "    df['quarter'] = df['datetime'].dt.quarter\n",
    "    df['quarter'] = df['quarter'].astype(int)\n",
    "    \n",
    "    df['month_start'] = df['day'].apply(lambda x: 1 if x<=7 else 0).astype(int)\n",
    "    df['month_end'] = df['day'].apply(lambda x: 1 if x>=25 else 0).astype(int)\n",
    "\n",
    "    df = label_transform(df, date_cate_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_datetime(train)\n",
    "test = process_datetime(test)\n",
    "cate_features += date_cate_features\n",
    "basic_num_features += date_num_features\n",
    "\n",
    "print('process_datetime done')\n",
    "\n",
    "\n",
    "################device####################\n",
    "device_features = ['browser','operatingSystem','isMobile','deviceCategory']\n",
    "\n",
    "def process_device(df):\n",
    "    df['browser'] = df['device'].apply(lambda x: json.loads(x)['browser'])\n",
    "    df['operatingSystem'] = df['device'].apply(lambda x: json.loads(x)['operatingSystem'])\n",
    "    df['isMobile'] = df['device'].apply(lambda x: json.loads(x)['isMobile'])\n",
    "    df['deviceCategory'] = df['device'].apply(lambda x: json.loads(x)['deviceCategory'])\n",
    "\n",
    "    df = label_transform(df, device_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_device(train)\n",
    "test = process_device(test)\n",
    "cate_features += device_features\n",
    "print('process_device done')\n",
    "\n",
    "\n",
    "###############geoNetwork#####################\n",
    "geo_features = ['continent','subContinent','country','region','metro','city','networkDomain']\n",
    "\n",
    "def process_geo(df):\n",
    "    df['continent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['continent'])\n",
    "    df['subContinent'] = df['geoNetwork'].apply(lambda x: json.loads(x)['subContinent'])\n",
    "    df['country'] = df['geoNetwork'].apply(lambda x: json.loads(x)['country'])\n",
    "    df['region'] = df['geoNetwork'].apply(lambda x: json.loads(x)['region'])\n",
    "    df['metro'] = df['geoNetwork'].apply(lambda x: json.loads(x)['metro'])\n",
    "    df['city'] = df['geoNetwork'].apply(lambda x: json.loads(x)['city'])\n",
    "    df['networkDomain'] = df['geoNetwork'].apply(lambda x: json.loads(x)['networkDomain'])\n",
    "\n",
    "    df = label_transform(df, geo_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_geo(train)\n",
    "test = process_geo(test)\n",
    "cate_features += geo_features\n",
    "print('process_geo done')\n",
    "\n",
    "\n",
    "################totals####################\n",
    "view_features = ['hits','pageviews','newVisits','bounces','visitNumber','transactionRevenue','buy_or_not']\n",
    "\n",
    "def process_totals(df):\n",
    "    df['hits'] = df['totals'].apply(lambda x: json.loads(x)['hits']).astype(int)\n",
    "    df['pageviews'] = df['totals'].apply(lambda x: json.loads(x)['pageviews'] if x.find('pageviews')>=0 else 0).astype(int)\n",
    "    df['bounces'] = df['totals'].apply(lambda x: json.loads(x)['bounces'] if x.find('bounces')>=0 else 0).astype(int)\n",
    "    df['newVisits'] = df['totals'].apply(lambda x: json.loads(x)['newVisits'] if x.find('newVisits')>=0 else 0).astype(int)\n",
    "    df['transactionRevenue'] = df['totals'].apply(lambda x: json.loads(x)['transactionRevenue'] if x.find('transactionRevenue')>=0 else 0).astype(int)\n",
    "    df['buy_or_not'] = df['transactionRevenue'].apply(lambda x: 1 if x>0 else 0).astype(int)\n",
    "    df['visitNumber'] = df['visitNumber'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_totals(train)\n",
    "test = process_totals(test)\n",
    "basic_num_features += view_features\n",
    "print('process_totals done')\n",
    "\n",
    "\n",
    "################totals####################\n",
    "last_time_features = ['last_seconds','last_minutes']\n",
    "\n",
    "def process_last_time(df):\n",
    "    df['last_seconds'] = df['visitStartTime']-df['visitId']\n",
    "    df['last_minutes'] = (df['visitStartTime']-df['visitId'])/60\n",
    "    df['last_minutes'] = df['last_minutes'].astype(np.int64)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_last_time(train)\n",
    "test = process_last_time(test)\n",
    "basic_num_features += last_time_features\n",
    "print('process_last_time done')\n",
    "\n",
    "\n",
    "def parse_adwordsClickInfo_field(x, field):\n",
    "    jo = json.loads(x)\n",
    "    \n",
    "    if x.find('adwordsClickInfo')>=0:\n",
    "        adwordsClickInfo = jo['adwordsClickInfo']\n",
    "        \n",
    "        if str(adwordsClickInfo).find(field)>=0:\n",
    "            return adwordsClickInfo[field]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def parse_adwordsClickInfo_page(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'page')\n",
    "\n",
    "def parse_adwordsClickInfo_slot(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'slot')\n",
    "\n",
    "def parse_adwordsClickInfo_gclId(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'gclId')\n",
    "\n",
    "def parse_adwordsClickInfo_adNetworkType(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'adNetworkType')\n",
    "\n",
    "def parse_adwordsClickInfo_isVideoAd(x):\n",
    "    return parse_adwordsClickInfo_field(x, 'isVideoAd')\n",
    "\n",
    "traffic_features = ['campaign','source','medium','keyword','adwordsClickInfo_gclId_prefix','adwordsClickInfo_slot',\n",
    "                    'adwordsClickInfo_gclId','adwordsClickInfo_adNetworkType']\n",
    "\n",
    "def process_traffic(df):\n",
    "    df['campaign'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['campaign']).astype(str)\n",
    "    # need to merge nearly same record\n",
    "    df['source'] = df['trafficSource'].apply(lambda x: json.loads(x)['source']).astype(str)\n",
    "    df['medium'] = df['trafficSource'].apply(lambda x: json.loads(x)['medium']).astype(str)\n",
    "    # need to merge some keywords\n",
    "    df['keyword'] = df['trafficSource'].progress_apply(lambda x: json.loads(x)['keyword'] if x.find('keyword')>=0 else 0).astype(str)\n",
    "\n",
    "    df['adwordsClickInfo_page'] = df['trafficSource'].apply(parse_adwordsClickInfo_page).astype(int)\n",
    "    df['adwordsClickInfo_slot'] = df['trafficSource'].apply(parse_adwordsClickInfo_slot).astype(str)\n",
    "    df['adwordsClickInfo_gclId'] = df['trafficSource'].apply(parse_adwordsClickInfo_gclId).astype(str)\n",
    "    df['adwordsClickInfo_gclId_prefix'] = df['adwordsClickInfo_gclId'].apply(lambda x: x.split('_')[0] if type(x)!=int and x.find('_')>=0 else 0).astype(str)\n",
    "    df['adwordsClickInfo_adNetworkType'] = df['trafficSource'].apply(parse_adwordsClickInfo_adNetworkType).astype(str)\n",
    "\n",
    "    df = label_transform(df, traffic_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_traffic(train)\n",
    "test = process_traffic(test)\n",
    "\n",
    "cate_features += traffic_features\n",
    "basic_num_features.append('adwordsClickInfo_page')\n",
    "print('process_traffic done')\n",
    "\n",
    "\n",
    "removed_columns = ['device','geoNetwork','socialEngagementType','totals','trafficSource']\n",
    "train.drop(removed_columns, axis=1, inplace=True)\n",
    "test.drop(removed_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "def print_mem(df):\n",
    "    print('-'*80)\n",
    "    usage_mb = df.memory_usage(deep=True) / 1024 ** 2\n",
    "    print(usage_mb)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day_seconds = 24 * 60* 60\n",
    "month_seconds = 30 * day_seconds\n",
    "debug=False\n",
    "\n",
    "transaction_features = ['days_from_last_visted','months_from_last_visted','last_visit_buy',\n",
    "                        'countOfRevenuePerVisitNumRate','max_visit_number','sum_previous_pageviews',\n",
    "                        'sum_previous_hits','transaction_count','buy_times']\n",
    "advance_num_features += transaction_features\n",
    "\n",
    "\n",
    "#################### Parallel run  #################### \n",
    "import tqdm\n",
    "\n",
    "#CreateFeature the seconds from last visited\n",
    "def create_feature_times_from_last_visited(group_df):\n",
    "    days_from_last_array = []\n",
    "    months_from_last_array = []\n",
    "    \n",
    "    visitId_array = group_df['visitId'].values\n",
    "    last = visitId_array[0]\n",
    "    for item in group_df['visitId'].values:\n",
    "        days = int((item-last)/day_seconds)\n",
    "        days_from_last_array.append(days)\n",
    "        \n",
    "        months = int((item-last)/month_seconds)\n",
    "        months_from_last_array.append(months)\n",
    "        \n",
    "        last = item\n",
    "        \n",
    "        \n",
    "    group_df['days_from_last_visted'] = days_from_last_array\n",
    "    group_df['months_from_last_visted'] = months_from_last_array\n",
    "    \n",
    "    return group_df\n",
    "    \n",
    "    \n",
    "    \n",
    "#CreateFeature does last visited bought?\n",
    "def create_feature_last_visited_bought(group_df):\n",
    "    last_deal_array = []\n",
    "    revenue_array = group_df['transactionRevenue'].values\n",
    "\n",
    "    last_revenue = revenue_array[0]\n",
    "    for i in range(len(revenue_array)):\n",
    "        if i==0:\n",
    "            last_deal_array.append(0)\n",
    "        else:\n",
    "            if revenue_array[i-1]>0:\n",
    "                last_deal_array.append(1)\n",
    "            else:\n",
    "                last_deal_array.append(0)\n",
    "\n",
    "    group_df['last_visit_buy'] = last_deal_array\n",
    "\n",
    "    return group_df\n",
    "    \n",
    "# CreateFeature \n",
    "# count(transactionRevenue)/MAX(visitNumber)\n",
    "def create_feature_countsOfvist_countsOfDeals(group_df):\n",
    "    max_visit_num = np.max(group_df['visitNumber'])\n",
    "    count_of_revenue = sum(group_df['transactionRevenue']>0)\n",
    "    \n",
    "    rate = count_of_revenue/max_visit_num\n",
    "    \n",
    "    group_df['countOfRevenuePerVisitNumRate'] = [rate]*group_df.shape[0]\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "def create_feature_max_visit_number(group_df):\n",
    "    max_visit_num = np.max(group_df['visitNumber'])\n",
    "    \n",
    "    group_df['max_visit_number'] = [max_visit_num]*group_df.shape[0]\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "def create_feature_previous_sum_pageviews(group_df):\n",
    "    sum_previous_pageviews = []\n",
    "    sum = 0\n",
    "    for item in group_df['pageviews'].values:\n",
    "        sum += item\n",
    "        sum_previous_pageviews.append(sum)\n",
    "            \n",
    "    group_df['sum_previous_pageviews'] = sum_previous_pageviews\n",
    "    \n",
    "    return group_df\n",
    "    \n",
    "def create_feature_previous_sum_hits(group_df):\n",
    "    sum_previous_hits = []\n",
    "    sum = 0\n",
    "    for item in group_df['hits'].values:\n",
    "        sum += item\n",
    "        sum_previous_hits.append(sum)\n",
    "            \n",
    "    group_df['sum_previous_hits'] = sum_previous_hits\n",
    "    \n",
    "    return group_df\n",
    "    \n",
    "def create_feature_transaction_count(group_df):\n",
    "    count = group_df.shape[0]\n",
    "    \n",
    "    group_df['transaction_count'] = [count] * group_df.shape[0]\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "def create_feature_buy_times(group_df):\n",
    "    buy_times = sum(group_df['transactionRevenue']>0)\n",
    "    \n",
    "    group_df['buy_times'] = [buy_times] * group_df.shape[0]\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "target = 'total_revenue'\n",
    "\n",
    "def create_feature_revenue(group_df):\n",
    "    sum_revenue = np.log(sum(group_df['transactionRevenue']) + 1)\n",
    "    \n",
    "    if debug is True:\n",
    "        print('method\\n',group_df.shape,group_df['transactionRevenue'].values,\n",
    "              sum(group_df['transactionRevenue']),sum_revenue)\n",
    "    \n",
    "    group_df[target] = [sum_revenue] * group_df.shape[0]\n",
    "    \n",
    "    return group_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel run applyParallelInBatch\n",
      "Run parallel_batch_process_groups\n",
      "apply async job to pool (112368, 8)\n",
      "apply async job to pool (112311, 8)\n",
      "apply async job to pool (112944, 8)\n",
      "apply async job to pool (112907, 8)\n",
      "apply async job to pool (112889, 8)\n",
      "Run parallel_batch_process_groups\n",
      "Run parallel_batch_process_groups\n",
      "apply async job to pool (113748, 8)\n",
      "apply async job to pool (112785, 8)\n",
      "apply async job to pool (112803, 8)\n",
      "\n",
      "\n",
      "Run parallel_batch_process_groups\n",
      "Run parallel_batch_process_groups\n",
      "Run parallel_batch_process_groups\n",
      "Run parallel_batch_process_groups\n",
      "Run parallel_batch_process_groups\n",
      "progress report ForkPoolWorker-42 5.612113185098717\n",
      "progress report ForkPoolWorker-41 5.58890267484882\n",
      "progress report ForkPoolWorker-43 5.576125262077887\n",
      "progress report ForkPoolWorker-45 5.593404257699321\n",
      "progress report ForkPoolWorker-44 5.619998201600575\n",
      "progress report ForkPoolWorker-46 5.584096493187403\n",
      "progress report ForkPoolWorker-47 5.608399138549893\n",
      "progress report ForkPoolWorker-48 5.624803131890384\n",
      "progress report ForkPoolWorker-41 11.17780534969764\n",
      "progress report ForkPoolWorker-42 11.224226370197433\n",
      "progress report ForkPoolWorker-43 11.152250524155773\n",
      "progress report ForkPoolWorker-45 11.186808515398642\n",
      "progress report ForkPoolWorker-44 11.23999640320115\n",
      "progress report ForkPoolWorker-46 11.168192986374805\n",
      "progress report ForkPoolWorker-47 11.216798277099786\n",
      "progress report ForkPoolWorker-48 11.249606263780768\n",
      "progress report ForkPoolWorker-41 16.76670802454646\n",
      "progress report ForkPoolWorker-42 16.83633955529615\n",
      "progress report ForkPoolWorker-45 16.780212773097965\n",
      "progress report ForkPoolWorker-43 16.728375786233663\n",
      "progress report ForkPoolWorker-46 16.752289479562208\n",
      "progress report ForkPoolWorker-44 16.859994604801727\n",
      "progress report ForkPoolWorker-47 16.825197415649676\n",
      "progress report ForkPoolWorker-48 16.87440939567115\n",
      "progress report ForkPoolWorker-45 22.373617030797284\n",
      "progress report ForkPoolWorker-42 22.448452740394867\n",
      "progress report ForkPoolWorker-41 22.35561069939528\n",
      "progress report ForkPoolWorker-43 22.304501048311547\n",
      "progress report ForkPoolWorker-46 22.33638597274961\n",
      "progress report ForkPoolWorker-44 22.4799928064023\n",
      "progress report ForkPoolWorker-47 22.43359655419957\n",
      "progress report ForkPoolWorker-48 22.499212527561536\n",
      "progress report ForkPoolWorker-42 28.060565925493584\n",
      "progress report ForkPoolWorker-45 27.967021288496607\n",
      "progress report ForkPoolWorker-41 27.9445133742441\n",
      "progress report ForkPoolWorker-43 27.880626310389438\n",
      "progress report ForkPoolWorker-46 27.920482465937013\n",
      "progress report ForkPoolWorker-44 28.099991008002878\n",
      "progress report ForkPoolWorker-47 28.04199569274946\n",
      "progress report ForkPoolWorker-48 28.124015659451917\n",
      "progress report ForkPoolWorker-45 33.56042554619593\n",
      "progress report ForkPoolWorker-42 33.6726791105923\n",
      "progress report ForkPoolWorker-41 33.53341604909292\n",
      "progress report ForkPoolWorker-43 33.456751572467326\n",
      "progress report ForkPoolWorker-46 33.504578959124416\n",
      "progress report ForkPoolWorker-47 33.65039483129935\n",
      "progress report ForkPoolWorker-44 33.719989209603455\n",
      "progress report ForkPoolWorker-48 33.7488187913423\n",
      "progress report ForkPoolWorker-45 39.15382980389525\n",
      "progress report ForkPoolWorker-42 39.28479229569102\n",
      "progress report ForkPoolWorker-41 39.12231872394174\n",
      "progress report ForkPoolWorker-43 39.03287683454521\n",
      "progress report ForkPoolWorker-46 39.08867545231182\n",
      "progress report ForkPoolWorker-47 39.25879396984925\n",
      "progress report ForkPoolWorker-44 39.33998741120403\n",
      "progress report ForkPoolWorker-48 39.37362192323269\n",
      "progress report ForkPoolWorker-45 44.74723406159457\n",
      "progress report ForkPoolWorker-42 44.896905480789734\n",
      "progress report ForkPoolWorker-41 44.71122139879056\n",
      "progress report ForkPoolWorker-43 44.609002096623094\n",
      "progress report ForkPoolWorker-46 44.67277194549922\n",
      "progress report ForkPoolWorker-47 44.86719310839914\n",
      "progress report ForkPoolWorker-44 44.9599856128046\n",
      "progress report ForkPoolWorker-48 44.99842505512307\n",
      "progress report ForkPoolWorker-42 50.50901866588845\n",
      "progress report ForkPoolWorker-45 50.34063831929389\n",
      "progress report ForkPoolWorker-41 50.30012407363939\n",
      "progress report ForkPoolWorker-43 50.18512735870099\n",
      "progress report ForkPoolWorker-46 50.25686843868662\n",
      "progress report ForkPoolWorker-47 50.47559224694903\n",
      "progress report ForkPoolWorker-44 50.57998381440518\n",
      "progress report ForkPoolWorker-48 50.62322818701346\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_one_group(group_df):\n",
    "    group_df = create_feature_times_from_last_visited(group_df)\n",
    "    group_df = create_feature_last_visited_bought(group_df)\n",
    "    group_df = create_feature_countsOfvist_countsOfDeals(group_df)\n",
    "    group_df = create_feature_max_visit_number(group_df)\n",
    "    \n",
    "#   pageviews\n",
    "    group_df = create_feature_previous_sum_pageviews(group_df)\n",
    "#   hits\n",
    "    group_df = create_feature_previous_sum_hits(group_df)\n",
    "#   transcation count\n",
    "    group_df = create_feature_transaction_count(group_df)\n",
    "#   buy times\n",
    "    group_df = create_feature_buy_times(group_df)\n",
    "#     sum of user revenue\n",
    "    group_df = create_feature_revenue(group_df)\n",
    "        \n",
    "    return group_df\n",
    "    \n",
    "\n",
    "def parallel_batch_process_groups(grouped):\n",
    "    print('Run parallel_batch_process_groups')\n",
    "    \n",
    "    batch = []\n",
    "    first_batch=True\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    counter = 0\n",
    "    total = len(grouped.size())\n",
    "    for name, group_df in grouped:\n",
    "        group_df_copy = group_df.copy()\n",
    "        \n",
    "        fullVisitorId = group_df_copy['fullVisitorId'].values[0]\n",
    "\n",
    "        group_df_copy.sort_values(by='visitNumber', inplace=True)\n",
    "        group_df_copy = process_one_group(group_df_copy)\n",
    "\n",
    "        batch.append(group_df_copy)\n",
    "        counter += 1\n",
    "        del group_df_copy\n",
    "        \n",
    "        if len(batch)>=5000:\n",
    "            print('progress report', multiprocessing.current_process().name,100.0*(counter/total))\n",
    "            if first_batch is True:\n",
    "                result_df = pd.concat(batch, axis=0)\n",
    "                del batch\n",
    "                batch = []\n",
    "                first_batch = False\n",
    "            else:\n",
    "                batch_df = pd.concat(batch, axis=0)\n",
    "                result_df = pd.concat([result_df, batch_df], axis=0)\n",
    "                del batch\n",
    "                del batch_df\n",
    "                batch = []\n",
    "        \n",
    "    batch_df = pd.concat(batch, axis=0)\n",
    "    \n",
    "    if result_df.shape[0]>0:\n",
    "        result_df = pd.concat([result_df, batch_df], axis=0)\n",
    "    else:\n",
    "        result_df = batch_df\n",
    "    del batch\n",
    "    del batch_df\n",
    "            \n",
    "    return result_df\n",
    "    \n",
    "\n",
    "def applyParallelInBatch(df, by_field, func):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    batch_size=8\n",
    "    \n",
    "    df_copy['hash_index'] = df_copy[by_field].apply(lambda x: hash(x) % batch_size)\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=batch_size)\n",
    "    \n",
    "    batch_result = []\n",
    "    for i in range(batch_size):\n",
    "        batch_df = df_copy[df_copy['hash_index'] == i]\n",
    "        grouped = batch_df.groupby(by_field)\n",
    "        \n",
    "        batch_result.append(pool.apply_async(func, args=(grouped,)))\n",
    "        print('apply async job to pool',batch_df.shape)\n",
    "        \n",
    "    print()\n",
    "    print()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=df.columns)\n",
    "    for i in batch_result:\n",
    "        part_df = i.get()\n",
    "        result_df = pd.concat([result_df,part_df], axis=0)\n",
    "        print('got result result_df',result_df.shape)\n",
    "            \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def process_transactions(df):\n",
    "    print('Parallel run applyParallelInBatch')\n",
    "    input_columns = ['fullVisitorId','visitId','visitNumber','transactionRevenue','pageviews','hits']\n",
    "    transaction_df = applyParallelInBatch(df[['sessionId'] + input_columns ],\n",
    "                                          'fullVisitorId',\n",
    "                                          parallel_batch_process_groups)\n",
    "    transaction_df.drop(input_columns, axis=1, inplace=True)\n",
    "\n",
    "    print('should have', len(df['fullVisitorId'].unique()))\n",
    "    print('transaction_df.shape',transaction_df.shape)\n",
    "\n",
    "    df = pd.merge(df,transaction_df, on='sessionId', how='left')\n",
    "    print_mem(df)\n",
    "\n",
    "    del transaction_df\n",
    "    \n",
    "    return df\n",
    "    \n",
    "train = process_transactions(train)\n",
    "test = process_transactions(test)\n",
    "\n",
    "print('process_transactions done')\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['fullVisitorId','transaction_count','transactionRevenue']+advance_num_features].loc[train['transaction_count']>1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['fullVisitorId']=='6664733704830724714'].T.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['total_revenue','countOfRevenuePerVisitNumRate']].loc[train['countOfRevenuePerVisitNumRate']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg numeric features\n",
    "def agg_numeric_feature(df):\n",
    "    for col in basic_num_features:\n",
    "        temp_df = df[[col,'fullVisitorId']].groupby('fullVisitorId')[col].agg(['min','max','mean','median'])\n",
    "        temp_df['fullVisitorId'] = temp_df.index\n",
    "        temp_df.rename(columns={\n",
    "            'min': col +'_' +'min',\n",
    "            'max': col +'_' +'max',\n",
    "            'mean': col +'_' +'mean',\n",
    "            'median': col +'_' +'median'\n",
    "        }, inplace=True)\n",
    "\n",
    "        df = pd.merge(df, temp_df, on='fullVisitorId')\n",
    "        del temp_df\n",
    "\n",
    "        print(col,df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = agg_numeric_feature(train)\n",
    "test = agg_numeric_feature(test)\n",
    "\n",
    "print('agg_numeric_feature done')\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg cate features\n",
    "from pandas import Series\n",
    "\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "\n",
    "def applyParallel(dfGrouped, func, total):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = list(tqdm.tqdm(\n",
    "            p.imap(func, [group for name, group in dfGrouped]), \n",
    "            total=total))\n",
    "\n",
    "    df = pd.concat(ret_list, axis=1)\n",
    "    del ret_list\n",
    "    \n",
    "    return df.T\n",
    "\n",
    "def agg_cate_group_func(group_df):\n",
    "    new_row = Series()\n",
    "    new_row['fullVisitorId'] = group_df['fullVisitorId'].values[0]\n",
    "\n",
    "    for col in cate_features:\n",
    "        value_counts_series = group_df[col].value_counts()\n",
    "        \n",
    "        counts_array = value_counts_series.values\n",
    "        length = len(counts_array)\n",
    "        if length == 1:\n",
    "            new_row[col + '_top_1'] = counts_array[0]\n",
    "            new_row[col + '_top_2'] = counts_array[0] \n",
    "            new_row[col + '_top_3'] = counts_array[0]\n",
    "        elif length == 2:\n",
    "            new_row[col + '_top_1'] = counts_array[0]\n",
    "            new_row[col + '_top_2'] = counts_array[1] \n",
    "            new_row[col + '_top_3'] = counts_array[1]\n",
    "        else:\n",
    "            new_row[col + '_top_1'] = counts_array[0]\n",
    "            new_row[col + '_top_2'] = counts_array[1] \n",
    "            new_row[col + '_top_3'] = counts_array[2]\n",
    "            \n",
    "        del value_counts_series\n",
    "        del counts_array\n",
    "    return new_row\n",
    "\n",
    "\n",
    "def expand_cate_features(df):\n",
    "    d_rows = df[df['fullVisitorId'].duplicated(keep=False)]\n",
    "    revisted_df = df.loc[d_rows.index]\n",
    "    \n",
    "    total = len(revisted_df['fullVisitorId'].unique())\n",
    "    print('revisted_df',total,revisted_df.shape)\n",
    "    \n",
    "    cate_group_data = revisted_df[cate_features + ['fullVisitorId']].groupby('fullVisitorId')\n",
    "\n",
    "    revisted_cates_top_3_df = applyParallel(cate_group_data, agg_cate_group_func, total)\n",
    "    print('got revisted_cates_top_3_df', revisted_cates_top_3_df.shape)\n",
    "\n",
    "    df = pd.merge(df, revisted_cates_top_3_df, on='fullVisitorId', how='left')\n",
    "    print('df merge cate_top_3_features_df done')\n",
    "    \n",
    "    del cate_group_data\n",
    "    del revisted_cates_top_3_df\n",
    "\n",
    "    return df\n",
    "    \n",
    "train = expand_cate_features(train)\n",
    "test = expand_cate_features(test)\n",
    "\n",
    "print('process expand_cate_features done')\n",
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "\n",
    "\n",
    "###################cate_features_fillna########################## \n",
    "def cate_features_fillna(df):\n",
    "    for col in cate_features:\n",
    "        top_1_null_index = df[df[col + '_top_1'].isnull()].index\n",
    "        df[col + '_top_1'].fillna(\n",
    "            df[col].loc[top_1_null_index], inplace=True)\n",
    "        \n",
    "        top_2_null_index = df[df[col + '_top_2'].isnull()].index\n",
    "        df[col + '_top_2'].fillna(\n",
    "            df[col].loc[top_2_null_index], inplace=True)\n",
    "        \n",
    "        top_3_null_index = df[df[col + '_top_3'].isnull()].index\n",
    "        df[col + '_top_3'].fillna(\n",
    "            df[col].loc[top_3_null_index], inplace=True)\n",
    "        print('cate_features_fillna', col)\n",
    "        \n",
    "        \n",
    "    return df\n",
    "        \n",
    "        \n",
    "train = cate_features_fillna(train)\n",
    "test = cate_features_fillna(test)\n",
    "print('process cate_features_fillna done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train.shape',train.shape)\n",
    "print('test.shape',test.shape)\n",
    "\n",
    "new_cate_features = [] \n",
    "for col in cate_features:\n",
    "    new_cate_features.append(col + '_top_1')\n",
    "    new_cate_features.append(col + '_top_2')\n",
    "    new_cate_features.append(col + '_top_3')\n",
    "    \n",
    "cate_features += new_cate_features\n",
    "\n",
    "\n",
    "new_numeric_features = []\n",
    "for col in basic_num_features:\n",
    "    new_numeric_features.append(col+'_' +'min')\n",
    "    new_numeric_features.append(col+'_' +'max')\n",
    "    new_numeric_features.append(col+'_' +'mean')\n",
    "    new_numeric_features.append(col+'_' +'median')\n",
    "    \n",
    "\n",
    "print()\n",
    "print('cate_features', len(cate_features), cate_features)\n",
    "print('basic_num_features', len(basic_num_features), basic_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_num_features += advance_num_features\n",
    "\n",
    "# basic_num_features += new_numeric_features\n",
    "\n",
    "# new_numeric_features\n",
    "# basic_num_features = list(set(basic_num_features)-set(new_numeric_features))\n",
    "\n",
    "print(basic_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[basic_num_features].T[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Dropout, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input_cate = Input((len(cate_features),))\n",
    "    input_numeric = Input((len(basic_num_features),))\n",
    "    \n",
    "    x_cate = Embedding(50000, 10)(input_cate)\n",
    "    x_cate = Flatten()(x_cate)\n",
    "    x_cate = Dropout(0.2)(x_cate)\n",
    "    x_cate = Dense(500, activation='relu')(x_cate)\n",
    "    \n",
    "    \n",
    "    x_numeric = Dense(500, activation='relu')(input_numeric)\n",
    "    x_numeric = Dropout(0.2)(x_numeric)\n",
    "    \n",
    "    x = concatenate([x_cate,x_numeric])\n",
    "    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    output = Dense(1, kernel_initializer='normal')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_cate,input_numeric], outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.fit([train[cate_features],train[basic_num_features]], train[target], validation_split=0.2, \n",
    "          epochs=3, batch_size=10)\n",
    "print('model.fit done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test.shape',test.shape)\n",
    "predict_test = model.predict([test[cate_features],test[basic_num_features]],verbose=1)\n",
    "\n",
    "test['PredictedLogRevenue'] = predict_test\n",
    "\n",
    "predict_df = test[['PredictedLogRevenue','fullVisitorId']].groupby('fullVisitorId').agg('mean')\n",
    "predict_df['fullVisitorId'] = predict_df.index\n",
    "\n",
    "\n",
    "predict_df[['fullVisitorId','PredictedLogRevenue']].to_csv('GStore_keras_single_row_model.csv', index=False)\n",
    "print(predict_df.shape)\n",
    "\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.loc[predict_df['PredictedLogRevenue']>1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
