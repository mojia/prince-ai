{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29999 entries, 0 to 29998\n",
      "Data columns (total 16 columns):\n",
      "id                                              29999 non-null object\n",
      "teacher_id                                      29999 non-null object\n",
      "teacher_prefix                                  29999 non-null object\n",
      "school_state                                    29999 non-null object\n",
      "project_submitted_datetime                      29999 non-null object\n",
      "project_grade_category                          29999 non-null object\n",
      "project_subject_categories                      29999 non-null object\n",
      "project_subject_subcategories                   29999 non-null object\n",
      "project_title                                   29999 non-null object\n",
      "project_essay_1                                 29999 non-null object\n",
      "project_essay_2                                 29999 non-null object\n",
      "project_essay_3                                 994 non-null object\n",
      "project_essay_4                                 994 non-null object\n",
      "project_resource_summary                        29999 non-null object\n",
      "teacher_number_of_previously_posted_projects    29999 non-null int64\n",
      "project_is_approved                             29999 non-null int64\n",
      "dtypes: int64(2), object(14)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "## reference: https://colab.research.google.com/drive/1QhSnbh-WJVGZjQJF8u974msOL_vAgMeS#scrollTo=PlAGuj5kuZm9\n",
    "## https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard import summary as summary_lib\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/DonorsChoose/'\n",
    "train_file='train_1.csv'\n",
    "eval_file='train_2.csv'\n",
    "eval_file='train_2.csv'\n",
    "test_file='test.csv'\n",
    "resource_file='resources.csv'\n",
    "SEED=1000\n",
    "positive_sample_size=30000\n",
    "\n",
    "train_df = pd.read_csv(dir + train_file)\n",
    "eval_df = pd.read_csv(dir + eval_file)\n",
    "test_df = pd.read_csv(dir + test_file)\n",
    "resource_df = pd.read_csv(dir + resource_file)\n",
    "label = LabelEncoder()\n",
    "low_memory=False\n",
    "\n",
    "\n",
    "def sampleData():\n",
    "    train_label_1_df = train_df[train_df['project_is_approved']==1].sample(n=positive_sample_size,\n",
    "                                                                           random_state=SEED)\n",
    "    train_label_0_df = train_df[train_df['project_is_approved']==0]\n",
    "\n",
    "    train_data = pd.concat([train_label_1_df,train_label_0_df])\n",
    "    train_data = shuffle(train_data)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "train_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "eval_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "test_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "\n",
    "train_data = train_df\n",
    "eval_data = eval_df\n",
    "test_data = test_df \n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_2_vec data shape: (29999, 200)\n",
      "text_2_vec data shape: (30000, 200)\n",
      "text_2_vec data shape: (78035, 200)\n",
      "text_2_vec data shape: (29999, 200)\n",
      "text_2_vec data shape: (30000, 200)\n",
      "text_2_vec data shape: (78035, 200)\n",
      "text_2_vec data shape: (29999, 200)\n",
      "text_2_vec data shape: (30000, 200)\n",
      "text_2_vec data shape: (78035, 200)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "\n",
    "vocab_size=30000\n",
    "embedding_size=50\n",
    "sentence_size=200\n",
    "\n",
    "FEATURES = ['teacher_id','teacher_prefix',\n",
    "            'school_state','project_grade_category',\n",
    "            'project_subject_categories','project_subject_subcategories',\n",
    "            'project_title',\n",
    "#           'teacher_number_of_previously_posted_projects'\n",
    "           ]\n",
    "teacher_id = tf.feature_column.categorical_column_with_hash_bucket('teacher_id', 1000)\n",
    "feature_columns.append(teacher_id)\n",
    "\n",
    "teacher_prefix = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"teacher_prefix\", [\n",
    "        \"Mrs.\",\"Ms.\",\"Mr.\",\"Teacher\",\"Dr.\"\n",
    "    ])\n",
    "feature_columns.append(teacher_prefix)\n",
    "\n",
    "school_state = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"school_state\",[\n",
    "        \"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\n",
    "        \"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\n",
    "        \"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\n",
    "        \"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"\n",
    "    ])\n",
    "feature_columns.append(school_state)\n",
    "\n",
    "# todo project_submitted_datetime\n",
    "\n",
    "project_grade_category = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_grade_category\",[\"Grades 3-5\",\"Grades 6-8\",\"Grades 9-12\",\"Grades PreK-2\"])\n",
    "feature_columns.append(project_grade_category)\n",
    "\n",
    "project_subject_categories = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_categories\",[\n",
    "       \"Applied Learning\",\"Health & Sports\",\"History & Civics\",\"Literacy & Language\",\n",
    "        \"Math & Science\",\"Music & The Arts\",\"Special Needs\",\"Warmth\"])\n",
    "feature_columns.append(project_subject_categories)\n",
    "\n",
    "\n",
    "project_subject_subcategories = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_subcategories\",[\n",
    "        \"Applied Learning\",\"Care & Hunger\",\"Health & Sports\",\"History & Civics\",\n",
    "        \"Literacy & Language\",\"Math & Science\",\"Music & The Arts\",\n",
    "        \"Warmth\",\"Applied Sciences\",\"Character Education\",\"Civics & Government\",\n",
    "        \"College & Career Prep\",\"Community Service\",\"ESL\",\"Early Development\",\n",
    "        \"Economics\",\"Environmental Science\",\"Extracurricular\",\"Financial Literacy\",\n",
    "        \"Foreign Languages\",\"Gym & Fitness\",\"Health & Life Science\",\"Health & Wellness\",\n",
    "        \"History & Geography\",\"Literacy\",\"Literature & Writing\",\"Mathematics\",\"Music\",\n",
    "        \"Nutrition Education\",\"Other\",\"Parent Involvement\",\"Performing Arts\",\n",
    "        \"Social Sciences\",\"Special Needs\",\"Team Sports\",\"Visual Arts\"])\n",
    "feature_columns.append(project_subject_subcategories)\n",
    "\n",
    "project_title = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'project_title', 5000)\n",
    "feature_columns.append(project_title)\n",
    "\n",
    "project_essay_1 = tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity('project_essay_1', vocab_size),\n",
    "                                                     dimension=embedding_size)\n",
    "feature_columns.append(project_essay_1)\n",
    "\n",
    "project_essay_2 = tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity('project_essay_2', vocab_size),\n",
    "                                                    dimension=embedding_size)\n",
    "feature_columns.append(project_essay_2)\n",
    "\n",
    "project_resource_summary = tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_identity('project_resource_summary',vocab_size),\n",
    "    dimension=embedding_size)\n",
    "feature_columns.append(project_resource_summary)\n",
    "\n",
    "teacher_number_of_previously_posted_projects = tf.feature_column.numeric_column('teacher_number_of_previously_posted_projects')\n",
    "# feature_columns.append(teacher_number_of_previously_posted_projects)\n",
    "\n",
    "\n",
    "target = 'project_is_approved'\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "\n",
    "def text_2_vec(feature_name, data_set):\n",
    "    texts = data_set[feature_name].values\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequence_data = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    data = sequence.pad_sequences(sequence_data, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "    print(\"text_2_vec data shape:\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "essay_1_train = text_2_vec('project_essay_1', train_data)\n",
    "essay_1_eval = text_2_vec('project_essay_1', eval_data)\n",
    "essay_1_test = text_2_vec('project_essay_1', test_data)\n",
    "\n",
    "essay_2_train = text_2_vec('project_essay_2', train_data)\n",
    "essay_2_eval = text_2_vec('project_essay_2', eval_data)\n",
    "essay_2_test = text_2_vec('project_essay_2', test_data)\n",
    "\n",
    "resource_summary_train = text_2_vec('project_resource_summary', train_data)\n",
    "resource_summary_eval = text_2_vec('project_resource_summary', eval_data)\n",
    "resource_summary_test = text_2_vec('project_resource_summary', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x132e5a2e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31472, step = 1\n",
      "INFO:tensorflow:global_step/sec: 13.2626\n",
      "INFO:tensorflow:loss = 50.938896, step = 101 (7.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.947\n",
      "INFO:tensorflow:loss = 32.659065, step = 201 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.356\n",
      "INFO:tensorflow:loss = 40.100746, step = 301 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.354\n",
      "INFO:tensorflow:loss = 42.88181, step = 401 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.048\n",
      "INFO:tensorflow:loss = 37.294567, step = 501 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.205\n",
      "INFO:tensorflow:loss = 43.04153, step = 601 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.802\n",
      "INFO:tensorflow:loss = 30.767784, step = 701 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.33\n",
      "INFO:tensorflow:loss = 31.266054, step = 801 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.304\n",
      "INFO:tensorflow:loss = 39.80178, step = 901 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.15\n",
      "INFO:tensorflow:loss = 31.818813, step = 1001 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.66\n",
      "INFO:tensorflow:loss = 43.579067, step = 1101 (0.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.354\n",
      "INFO:tensorflow:loss = 33.237568, step = 1201 (0.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.46\n",
      "INFO:tensorflow:loss = 29.054617, step = 1301 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.567\n",
      "INFO:tensorflow:loss = 34.01994, step = 1401 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.566\n",
      "INFO:tensorflow:loss = 36.911255, step = 1501 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.61\n",
      "INFO:tensorflow:loss = 31.512022, step = 1601 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.469\n",
      "INFO:tensorflow:loss = 28.84815, step = 1701 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.86\n",
      "INFO:tensorflow:loss = 30.641554, step = 1801 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.392\n",
      "INFO:tensorflow:loss = 32.34942, step = 1901 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.42\n",
      "INFO:tensorflow:loss = 25.6971, step = 2001 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.773\n",
      "INFO:tensorflow:loss = 40.601086, step = 2101 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.198\n",
      "INFO:tensorflow:loss = 28.808237, step = 2201 (0.636 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.428\n",
      "INFO:tensorflow:loss = 29.43388, step = 2301 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.239\n",
      "INFO:tensorflow:loss = 37.319477, step = 2401 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.955\n",
      "INFO:tensorflow:loss = 24.648376, step = 2501 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.462\n",
      "INFO:tensorflow:loss = 29.006641, step = 2601 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.565\n",
      "INFO:tensorflow:loss = 36.473934, step = 2701 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.601\n",
      "INFO:tensorflow:loss = 28.922989, step = 2801 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.812\n",
      "INFO:tensorflow:loss = 21.94551, step = 2901 (0.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.308\n",
      "INFO:tensorflow:loss = 37.862843, step = 3001 (0.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.425\n",
      "INFO:tensorflow:loss = 25.203564, step = 3101 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.358\n",
      "INFO:tensorflow:loss = 23.345852, step = 3201 (0.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.898\n",
      "INFO:tensorflow:loss = 31.089123, step = 3301 (0.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.913\n",
      "INFO:tensorflow:loss = 19.929167, step = 3401 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.345\n",
      "INFO:tensorflow:loss = 26.168724, step = 3501 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.931\n",
      "INFO:tensorflow:loss = 25.656227, step = 3601 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.175\n",
      "INFO:tensorflow:loss = 24.585989, step = 3701 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.275\n",
      "INFO:tensorflow:loss = 26.554771, step = 3801 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.893\n",
      "INFO:tensorflow:loss = 30.503925, step = 3901 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.203\n",
      "INFO:tensorflow:loss = 24.125675, step = 4001 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.234\n",
      "INFO:tensorflow:loss = 23.829836, step = 4101 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.378\n",
      "INFO:tensorflow:loss = 31.932407, step = 4201 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.677\n",
      "INFO:tensorflow:loss = 25.655333, step = 4301 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.729\n",
      "INFO:tensorflow:loss = 21.480766, step = 4401 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.873\n",
      "INFO:tensorflow:loss = 31.619871, step = 4501 (0.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.77\n",
      "INFO:tensorflow:loss = 24.099627, step = 4601 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.1\n",
      "INFO:tensorflow:loss = 19.907776, step = 4701 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.641\n",
      "INFO:tensorflow:loss = 28.172798, step = 4801 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.742\n",
      "INFO:tensorflow:loss = 26.881073, step = 4901 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.691\n",
      "INFO:tensorflow:loss = 24.263191, step = 5001 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.125\n",
      "INFO:tensorflow:loss = 30.182476, step = 5101 (0.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.462\n",
      "INFO:tensorflow:loss = 26.315525, step = 5201 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.741\n",
      "INFO:tensorflow:loss = 24.336151, step = 5301 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.284\n",
      "INFO:tensorflow:loss = 33.830223, step = 5401 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.391\n",
      "INFO:tensorflow:loss = 22.912186, step = 5501 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.925\n",
      "INFO:tensorflow:loss = 19.003939, step = 5601 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.86\n",
      "INFO:tensorflow:loss = 22.012817, step = 5701 (0.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.455\n",
      "INFO:tensorflow:loss = 20.229097, step = 5801 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.938\n",
      "INFO:tensorflow:loss = 19.551697, step = 5901 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.516\n",
      "INFO:tensorflow:loss = 16.45908, step = 6001 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.909\n",
      "INFO:tensorflow:loss = 14.805338, step = 6101 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.85\n",
      "INFO:tensorflow:loss = 19.006006, step = 6201 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.247\n",
      "INFO:tensorflow:loss = 28.23098, step = 6301 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.462\n",
      "INFO:tensorflow:loss = 15.479622, step = 6401 (0.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.872\n",
      "INFO:tensorflow:loss = 19.462652, step = 6501 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.308\n",
      "INFO:tensorflow:loss = 22.0466, step = 6601 (0.670 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.993\n",
      "INFO:tensorflow:loss = 23.300613, step = 6701 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.001\n",
      "INFO:tensorflow:loss = 19.739447, step = 6801 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.144\n",
      "INFO:tensorflow:loss = 28.422403, step = 6901 (0.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.031\n",
      "INFO:tensorflow:loss = 15.30898, step = 7001 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.014\n",
      "INFO:tensorflow:loss = 17.10517, step = 7101 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.909\n",
      "INFO:tensorflow:loss = 23.72552, step = 7201 (0.614 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 158.094\n",
      "INFO:tensorflow:loss = 21.553125, step = 7301 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.653\n",
      "INFO:tensorflow:loss = 16.124584, step = 7401 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.35\n",
      "INFO:tensorflow:loss = 23.005424, step = 7501 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.608\n",
      "INFO:tensorflow:loss = 16.973589, step = 7601 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.93\n",
      "INFO:tensorflow:loss = 17.339565, step = 7701 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.339\n",
      "INFO:tensorflow:loss = 22.24163, step = 7801 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.915\n",
      "INFO:tensorflow:loss = 19.25206, step = 7901 (0.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.987\n",
      "INFO:tensorflow:loss = 18.71869, step = 8001 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.433\n",
      "INFO:tensorflow:loss = 22.595303, step = 8101 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.706\n",
      "INFO:tensorflow:loss = 18.179087, step = 8201 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.204\n",
      "INFO:tensorflow:loss = 13.025692, step = 8301 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.552\n",
      "INFO:tensorflow:loss = 21.350039, step = 8401 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.004\n",
      "INFO:tensorflow:loss = 19.994362, step = 8501 (0.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.043\n",
      "INFO:tensorflow:loss = 11.067781, step = 8601 (0.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.534\n",
      "INFO:tensorflow:loss = 24.761683, step = 8701 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.84\n",
      "INFO:tensorflow:loss = 17.441082, step = 8801 (0.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.369\n",
      "INFO:tensorflow:loss = 13.175068, step = 8901 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.504\n",
      "INFO:tensorflow:loss = 19.134306, step = 9001 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.295\n",
      "INFO:tensorflow:loss = 14.801029, step = 9101 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.309\n",
      "INFO:tensorflow:loss = 10.175442, step = 9201 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.926\n",
      "INFO:tensorflow:loss = 18.446215, step = 9301 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.312\n",
      "INFO:tensorflow:loss = 7.790246, step = 9401 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.74\n",
      "INFO:tensorflow:loss = 7.976751, step = 9501 (0.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.879\n",
      "INFO:tensorflow:loss = 15.883838, step = 9601 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.995\n",
      "INFO:tensorflow:loss = 16.945538, step = 9701 (0.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.867\n",
      "INFO:tensorflow:loss = 16.95423, step = 9801 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.103\n",
      "INFO:tensorflow:loss = 18.288927, step = 9901 (0.644 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 14.557271.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-31-17:25:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-31-17:25:15\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.79546666, accuracy_baseline = 0.8486, auc = 0.53567225, auc_precision_recall = 0.88240147, average_loss = 0.73041236, global_step = 10000, label/mean = 0.8486, loss = 73.04124, precision = 0.84965616, prediction/mean = 0.879127, recall = 0.92214626\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt-10000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "train_data_features = train_data[FEATURES].values\n",
    "eval_data_features = eval_data[FEATURES].values\n",
    "test_data_features = test_data[FEATURES].values\n",
    "\n",
    "def parser(essay_1, essay_2, resource_summary, data_set, y):\n",
    "    features = {k:data_set[:, FEATURES.index(k)] for k in FEATURES}\n",
    "\n",
    "    features[\"project_essay_1\"] = essay_1\n",
    "    features[\"project_essay_2\"] = essay_2\n",
    "    features[\"project_resource_summary\"] = resource_summary\n",
    "    \n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    y_train = train_data[target]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_train, essay_2_train,\n",
    "                                                  resource_summary_train, train_data_features,\n",
    "                                                  y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    return iterator.get_next()\n",
    "    \n",
    "def eval_input_fn():\n",
    "    y_eval = eval_data[target]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_eval,essay_2_eval,\n",
    "                                                  resource_summary_eval, eval_data_features,\n",
    "                                                  y_eval))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def test_input_fn():\n",
    "    y_test = test_data[target]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_test,essay_2_test,\n",
    "                                                  resource_summary_test, test_data_features,\n",
    "                                                  y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps = 10000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(\n",
    "        input_fn=eval_input_fn)])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, \n",
    "                              labels = eval_data[target].astype(bool),\n",
    "                              num_thresholds = 21)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'),\n",
    "                                      sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step = 0)\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n",
    "                                           model_dir=os.path.join(model_dir, 'bow_sparse'))\n",
    "\n",
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 78035\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[0.9698241  0.860133   0.99670154 0.9966903  0.9979532  0.99721354\n",
      " 0.6550113  0.994561   0.9991621  0.18014787]\n"
     ]
    }
   ],
   "source": [
    "def predict_input_fn():\n",
    "    length = len(test_data_features[:, 1])\n",
    "    print('length',length)\n",
    "    y_test = np.empty([length,1])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_test,essay_2_test,\n",
    "                                                  resource_summary_test, test_data_features,\n",
    "                                                  y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "predictions = np.array([p['logistic'][0] for p in classifier.predict(\n",
    "    input_fn=predict_input_fn)])\n",
    "\n",
    "print(predictions[:10])\n",
    "\n",
    "predict_result = pd.DataFrame({\n",
    "    \"id\":test_data['id'],\n",
    "    \"project_is_approved\":predictions\n",
    "})\n",
    "predict_result.to_csv('prince_baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
