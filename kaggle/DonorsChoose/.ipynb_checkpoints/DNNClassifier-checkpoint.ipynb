{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p000001</td>\n",
       "      <td>Cap Barbell 300 Pound Olympic Set, Grey Cap Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p000002</td>\n",
       "      <td>10 Sony Headphones (BUY 9 GET 1 FREE) Belkin 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p000003</td>\n",
       "      <td>EE820X - Phonemic Awareness Instant Learning C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p000004</td>\n",
       "      <td>A Bad Case of the Giggles Poems That Will Make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p000005</td>\n",
       "      <td>Fitbit Zip Wireless Activity Tracker, Lime Fit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description\n",
       "0  p000001  Cap Barbell 300 Pound Olympic Set, Grey Cap Ba...\n",
       "1  p000002  10 Sony Headphones (BUY 9 GET 1 FREE) Belkin 6...\n",
       "2  p000003  EE820X - Phonemic Awareness Instant Learning C...\n",
       "3  p000004  A Bad Case of the Giggles Poems That Will Make...\n",
       "4  p000005  Fitbit Zip Wireless Activity Tracker, Lime Fit..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard import summary as summary_lib\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/DonorsChoose/'\n",
    "train_file = dir + 'train.csv'\n",
    "test_file = dir + 'test.csv'\n",
    "resource_file = dir + 'resources.csv'\n",
    "model_dir = \"/Users/xinwang/Downloads/models_temp/\"\n",
    "label = LabelEncoder()\n",
    "\n",
    "CSV_COLUMNS = ['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
    "       'project_submitted_datetime', 'project_grade_category',\n",
    "       'project_subject_categories', 'project_subject_subcategories',\n",
    "       'project_title', 'project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
    "       'teacher_number_of_previously_posted_projects', 'project_is_approved']\n",
    "TEST_CSV_COLUMNS = ['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
    "       'project_submitted_datetime', 'project_grade_category',\n",
    "       'project_subject_categories', 'project_subject_subcategories',\n",
    "       'project_title', 'project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
    "       'teacher_number_of_previously_posted_projects']\n",
    "RESOURCE_COLUMNS = ['id', 'description', 'quantity', 'price']\n",
    "target = 'project_is_approved'\n",
    "\n",
    "Train_Mode = 'train'\n",
    "Eval_Mode = 'eval'\n",
    "Test_Mode = 'test'\n",
    "\n",
    "positive_sample_size=5000\n",
    "negative_sample_size=5000\n",
    "\n",
    "summary_vocab_size=10000\n",
    "summary_sentence_size=50\n",
    "summary_embedding_size=20\n",
    "\n",
    "essay_vocab_size=30000\n",
    "essay_sentence_size=200\n",
    "essay_embedding_size=20\n",
    "pad_id=0\n",
    "\n",
    "\n",
    "resource_df = pd.read_csv(resource_file)\n",
    "groups = resource_df[['id','description']].groupby('id', as_index=False)\n",
    "\n",
    "id_column = []\n",
    "desc_column = []\n",
    "for name, group in groups:\n",
    "    id_column.append(name)\n",
    "\n",
    "    desc = ' '.join(str(k) if type(k)==int or type(k)==float else k for k in group['description'].values)\n",
    "    desc_column.append(desc)\n",
    "    \n",
    "desc_df = pd.DataFrame({\n",
    "    \"id\":id_column,\n",
    "    \"description\": desc_column\n",
    "})\n",
    "\n",
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile done\n"
     ]
    }
   ],
   "source": [
    "SEED = 100\n",
    "train_epoches = 5000\n",
    "hidden_layers = [100, 50, 100, 50]\n",
    "\n",
    "teacher_id = tf.feature_column.categorical_column_with_hash_bucket('teacher_id', hash_bucket_size=1000)\n",
    "\n",
    "project_title = tf.feature_column.categorical_column_with_hash_bucket('project_title', hash_bucket_size=5000)\n",
    "\n",
    "teacher_prefix = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"teacher_prefix\", [\n",
    "        \"Mrs.\",\"Ms.\",\"Mr.\",\"Teacher\",\"Dr.\"\n",
    "    ])\n",
    "teacher_prefix_bins = tf.feature_column.numeric_column('teacher_prefix_bins')\n",
    "\n",
    "school_state = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"school_state\",[\n",
    "        \"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\n",
    "        \"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\n",
    "        \"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\n",
    "        \"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"\n",
    "    ])\n",
    "school_state_bins = tf.feature_column.numeric_column('school_state_bins')\n",
    "\n",
    "project_grade_category = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_grade_category\",[\"Grades 3-5\",\"Grades 6-8\",\"Grades 9-12\",\"Grades PreK-2\"])\n",
    "project_grade_category_bins = tf.feature_column.numeric_column('project_grade_category_bins')\n",
    "\n",
    "project_subject_categories = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_categories\",[\n",
    "       \"Applied Learning\",\"Health & Sports\",\"History & Civics\",\"Literacy & Language\",\n",
    "        \"Math & Science\",\"Music & The Arts\",\"Special Needs\",\"Warmth\"])\n",
    "project_subject_categories_bins = tf.feature_column.numeric_column('project_subject_categories_bins')\n",
    "\n",
    "project_subject_subcategories = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_subcategories\",[\n",
    "        \"Applied Learning\",\"Care & Hunger\",\"Health & Sports\",\"History & Civics\",\n",
    "        \"Literacy & Language\",\"Math & Science\",\"Music & The Arts\",\n",
    "        \"Warmth\",\"Applied Sciences\",\"Character Education\",\"Civics & Government\",\n",
    "        \"College & Career Prep\",\"Community Service\",\"ESL\",\"Early Development\",\n",
    "        \"Economics\",\"Environmental Science\",\"Extracurricular\",\"Financial Literacy\",\n",
    "        \"Foreign Languages\",\"Gym & Fitness\",\"Health & Life Science\",\"Health & Wellness\",\n",
    "        \"History & Geography\",\"Literacy\",\"Literature & Writing\",\"Mathematics\",\"Music\",\n",
    "        \"Nutrition Education\",\"Other\",\"Parent Involvement\",\"Performing Arts\",\n",
    "        \"Social Sciences\",\"Special Needs\",\"Team Sports\",\"Visual Arts\"])\n",
    "project_subject_subcategories_bins = tf.feature_column.numeric_column('project_subject_subcategories_bins')\n",
    "\n",
    "posted_projects = tf.feature_column.numeric_column('teacher_number_of_previously_posted_projects')\n",
    "posted_projects_bins = tf.feature_column.numeric_column('posted_projects_bins')\n",
    "\n",
    "quantity = tf.feature_column.numeric_column('quantity')\n",
    "price = tf.feature_column.numeric_column('price')\n",
    "avgPrice = tf.feature_column.numeric_column('avgPrice')\n",
    "\n",
    "\n",
    "#################### Text columns #########################\n",
    "project_essay_1_vec = tf.feature_column.categorical_column_with_identity('project_essay_1_vec',essay_vocab_size)\n",
    "project_essay_2_vec = tf.feature_column.categorical_column_with_identity('project_essay_2_vec',essay_vocab_size)\n",
    "summary_vec = tf.feature_column.categorical_column_with_identity('project_resource_summary_vec', summary_vocab_size)\n",
    "description_vec = tf.feature_column.categorical_column_with_identity('description_vec', summary_vocab_size)\n",
    "\n",
    "\n",
    "basic_columns = [\n",
    "\n",
    "]\n",
    "\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(['teacher_prefix', 'school_state'],\n",
    "                                    hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(['school_state', 'project_grade_category'],\n",
    "                                    hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(['project_grade_category', 'project_subject_categories'],\n",
    "                                    hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(['project_subject_categories', 'project_subject_subcategories'],\n",
    "                                   hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(['project_subject_categories', 'posted_projects_bins'],\n",
    "                                   hash_bucket_size=10000)\n",
    "]\n",
    "deep_columns = [\n",
    "    teacher_prefix_bins,\n",
    "    school_state_bins,\n",
    "    posted_projects,\n",
    "    project_grade_category_bins,\n",
    "    project_subject_categories_bins,\n",
    "    project_subject_subcategories_bins,\n",
    "    posted_projects_bins,\n",
    "    quantity,\n",
    "    price,\n",
    "    avgPrice,\n",
    "        \n",
    "    tf.feature_column.indicator_column(teacher_id),\n",
    "    tf.feature_column.indicator_column(project_title),\n",
    "    tf.feature_column.indicator_column(teacher_prefix),\n",
    "    tf.feature_column.indicator_column(school_state),\n",
    "    tf.feature_column.indicator_column(project_grade_category),\n",
    "    tf.feature_column.indicator_column(project_subject_categories),\n",
    "    tf.feature_column.indicator_column(project_subject_subcategories),\n",
    "    \n",
    "    tf.feature_column.embedding_column(project_essay_1_vec, dimension=essay_embedding_size),\n",
    "    tf.feature_column.embedding_column(project_essay_2_vec, dimension=essay_embedding_size),\n",
    "    tf.feature_column.embedding_column(summary_vec, dimension=summary_embedding_size),\n",
    "    tf.feature_column.embedding_column(description_vec, dimension=20),\n",
    "]\n",
    "\n",
    "def mix_operation(a, b):\n",
    "    return str(a) + '_' + str(b)\n",
    "\n",
    "def text_2_vec(feature_name, data_set, vocab_size, sentence_size):\n",
    "    texts = data_set[feature_name].values\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequence_data = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    data = sequence.pad_sequences(sequence_data, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "    print(feature_name + \" text_2_vec data shape:\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def build_input_features_dict(input_df):\n",
    "    features = {}\n",
    "    features['teacher_id'] = input_df['teacher_id'].values\n",
    "    \n",
    "    features['project_title'] = input_df['project_title'].values\n",
    "    \n",
    "    features['teacher_prefix'] = input_df['teacher_prefix'].values\n",
    "    features['teacher_prefix_bins'] = input_df['teacher_prefix_bins'].values\n",
    "    \n",
    "    features['school_state'] = input_df['school_state'].values\n",
    "    features['school_state_bins'] = input_df['school_state_bins'].values\n",
    "    \n",
    "    features['project_grade_category'] = input_df['project_grade_category'].values\n",
    "    features['project_grade_category_bins'] = input_df['project_grade_category_bins'].values\n",
    "\n",
    "    features['project_subject_categories'] = input_df['project_subject_categories'].values\n",
    "    features['project_subject_categories_bins'] = input_df['project_subject_categories_bins'].values\n",
    "\n",
    "    features['project_subject_subcategories'] = input_df['project_subject_subcategories'].values\n",
    "    features['project_subject_subcategories_bins'] = input_df['project_subject_subcategories_bins'].values\n",
    "    \n",
    "    features['teacher_number_of_previously_posted_projects'] = input_df['teacher_number_of_previously_posted_projects'].values\n",
    "    features['posted_projects_bins'] = input_df['posted_projects_bins'].values\n",
    "    \n",
    "    features['quantity'] = input_df['quantity'].values\n",
    "    features['price'] = input_df['price'].values\n",
    "    features['avgPrice'] = input_df['price']/input_df['quantity']\n",
    "    \n",
    "    #---  Text columns  ---#\n",
    "    features['project_essay_2_vec'] = text_2_vec('project_essay_2',input_df, \n",
    "                                                          essay_vocab_size, essay_sentence_size)\n",
    "    features['project_essay_1_vec'] = text_2_vec('project_essay_1',input_df, \n",
    "                                                          essay_vocab_size, essay_sentence_size)\n",
    "    features['project_resource_summary_vec'] = text_2_vec('project_resource_summary',input_df, \n",
    "                                                          summary_vocab_size,summary_sentence_size)\n",
    "    features['description_vec'] = text_2_vec('description',input_df, summary_vocab_size, 100)\n",
    "    #--- Text columns  ---#\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def balanceData(df):\n",
    "    label_1_df = df[df['project_is_approved']==1].sample(n=positive_sample_size, random_state=SEED)\n",
    "    label_0_df = df[df['project_is_approved']==0].sample(n=negative_sample_size, random_state=SEED)\n",
    "\n",
    "    data_set = pd.concat([label_1_df,label_0_df])\n",
    "    data_set = shuffle(data_set)\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "\n",
    "all_df = pd.read_csv(\n",
    "        tf.gfile.Open(train_file),names=CSV_COLUMNS,\n",
    "        skipinitialspace=True,engine=\"python\",skiprows=1)\n",
    "balance_df = balanceData(all_df)\n",
    "\n",
    "train_df = balance_df.sample(frac=0.8, random_state=SEED)\n",
    "eval_df = balance_df.drop(train_df.index)\n",
    "def build_input_df(file, mode):\n",
    "    if mode == Test_Mode:\n",
    "        test_df = pd.read_csv(tf.gfile.Open(test_file),names=TEST_CSV_COLUMNS,\n",
    "                              skipinitialspace=True,engine=\"python\",skiprows=1)\n",
    "        \n",
    "        print('get Test Mode dataset', test_df.shape)\n",
    "        return test_df\n",
    "    elif mode == Eval_Mode:\n",
    "        print('get Eval Mode dataset', eval_df.shape)\n",
    "        return eval_df\n",
    "    else:\n",
    "        print('get Train Mode dataset', train_df.shape)\n",
    "        return train_df\n",
    "\n",
    "def input_fn(file, mode, num_epochs, shuffle):\n",
    "    input_df = build_input_df(file, mode)\n",
    "    resource_df = pd.read_csv(\n",
    "        tf.gfile.Open(resource_file),names=RESOURCE_COLUMNS,\n",
    "        skipinitialspace=True,engine=\"python\",skiprows=1)\n",
    "    \n",
    "    input_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "    input_df['teacher_prefix_bins'] = label.fit_transform(input_df['teacher_prefix'])\n",
    "\n",
    "    input_df['school_state_bins'] = label.fit_transform(input_df['school_state'])\n",
    "    input_df['project_grade_category_bins'] = label.fit_transform(input_df['project_grade_category'])\n",
    "    input_df['project_subject_categories_bins'] = label.fit_transform(input_df['project_subject_categories'])\n",
    "    input_df['project_subject_subcategories_bins'] = label.fit_transform(input_df['project_subject_subcategories'])\n",
    "    \n",
    "    input_df['posted_projects_periods'] = pd.cut(input_df['teacher_number_of_previously_posted_projects'], \n",
    "                                                 [-1,7,50,1000])\n",
    "    input_df['posted_projects_bins'] = label.fit_transform(input_df['posted_projects_periods'])\n",
    "    \n",
    "    total_quantity_df = resource_df[['id','quantity']].groupby('id', as_index=False).sum()\n",
    "    total_price_df = resource_df[['id','price']].groupby('id', as_index=False).sum()\n",
    "\n",
    "    \n",
    "#   add quantity column, price column and divided of both columns from resource file into input_df \n",
    "    input_df = pd.merge(input_df, total_quantity_df, how='inner', on='id')\n",
    "    input_df = pd.merge(input_df, total_price_df, how='inner', on='id')\n",
    "    input_df = pd.merge(input_df, desc_df, how='inner', on='id')\n",
    "\n",
    "     \n",
    "    features = build_input_features_dict(input_df)\n",
    "\n",
    "    length = len(input_df.iloc[:, 1])\n",
    "    print(mode + ' dataset length',length)\n",
    "    \n",
    "    threads = 8\n",
    "    if mode == Test_Mode:\n",
    "        input_df.info()\n",
    "        labels = np.empty([length,1])\n",
    "        threads = 1\n",
    "    else:\n",
    "        input_df.info()\n",
    "        labels = input_df[target].values\n",
    "    \n",
    "    print(mode + ' labels size:' + str(len(labels)))\n",
    "    \n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(features,\n",
    "                                              labels,\n",
    "                                              batch_size=100,\n",
    "                                              num_epochs=num_epochs,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_threads=threads)\n",
    "\n",
    "def buildClassifier():\n",
    "    m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        linear_feature_columns = basic_columns + crossed_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units=hidden_layers)\n",
    "    return m\n",
    "    \n",
    "def train_and_evaluate():\n",
    "    classifier = buildClassifier()\n",
    "\n",
    "    classifier.train(input_fn=input_fn(train_file, Train_Mode, num_epochs=None, shuffle=True), \n",
    "                     steps = train_epoches)\n",
    "    results = classifier.evaluate(input_fn=input_fn(train_file, Eval_Mode, num_epochs=1, shuffle=False),\n",
    "                                      steps=None)\n",
    "    \n",
    "    print('-'*100)\n",
    "    for key in sorted(results):\n",
    "        print(\"%s: %s\" % (key, results[key]))\n",
    "        \n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "print('compile done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x148dba198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "get Train Mode dataset (8000, 16)\n",
      "project_essay_2 text_2_vec data shape: (8000, 200)\n",
      "project_essay_1 text_2_vec data shape: (8000, 200)\n",
      "project_resource_summary text_2_vec data shape: (8000, 50)\n",
      "description text_2_vec data shape: (8000, 100)\n",
      "train dataset length 8000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8000 entries, 0 to 7999\n",
      "Data columns (total 26 columns):\n",
      "id                                              8000 non-null object\n",
      "teacher_id                                      8000 non-null object\n",
      "teacher_prefix                                  8000 non-null object\n",
      "school_state                                    8000 non-null object\n",
      "project_submitted_datetime                      8000 non-null object\n",
      "project_grade_category                          8000 non-null object\n",
      "project_subject_categories                      8000 non-null object\n",
      "project_subject_subcategories                   8000 non-null object\n",
      "project_title                                   8000 non-null object\n",
      "project_essay_1                                 8000 non-null object\n",
      "project_essay_2                                 8000 non-null object\n",
      "project_essay_3                                 235 non-null object\n",
      "project_essay_4                                 235 non-null object\n",
      "project_resource_summary                        8000 non-null object\n",
      "teacher_number_of_previously_posted_projects    8000 non-null int64\n",
      "project_is_approved                             8000 non-null int64\n",
      "teacher_prefix_bins                             8000 non-null int64\n",
      "school_state_bins                               8000 non-null int64\n",
      "project_grade_category_bins                     8000 non-null int64\n",
      "project_subject_categories_bins                 8000 non-null int64\n",
      "project_subject_subcategories_bins              8000 non-null int64\n",
      "posted_projects_periods                         8000 non-null category\n",
      "posted_projects_bins                            8000 non-null int64\n",
      "quantity                                        8000 non-null int64\n",
      "price                                           8000 non-null float64\n",
      "description                                     8000 non-null object\n",
      "dtypes: category(1), float64(1), int64(9), object(15)\n",
      "memory usage: 1.6+ MB\n",
      "train labels size:8000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx/model.ckpt.\n",
      "INFO:tensorflow:loss = 83.7395, step = 1\n",
      "INFO:tensorflow:global_step/sec: 42.0657\n",
      "INFO:tensorflow:loss = 67.68117, step = 101 (2.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.6147\n",
      "INFO:tensorflow:loss = 67.82575, step = 201 (1.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.7837\n",
      "INFO:tensorflow:loss = 64.53805, step = 301 (1.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.0915\n",
      "INFO:tensorflow:loss = 64.49087, step = 401 (1.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.2348\n",
      "INFO:tensorflow:loss = 65.774055, step = 501 (1.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.5836\n",
      "INFO:tensorflow:loss = 63.632557, step = 601 (1.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.7045\n",
      "INFO:tensorflow:loss = 60.379265, step = 701 (1.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7128\n",
      "INFO:tensorflow:loss = 62.161564, step = 801 (1.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.0105\n",
      "INFO:tensorflow:loss = 64.19904, step = 901 (1.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7366\n",
      "INFO:tensorflow:loss = 64.66491, step = 1001 (1.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.5453\n",
      "INFO:tensorflow:loss = 63.13858, step = 1101 (1.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.0978\n",
      "INFO:tensorflow:loss = 63.425835, step = 1201 (1.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.6767\n",
      "INFO:tensorflow:loss = 65.89038, step = 1301 (1.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.3973\n",
      "INFO:tensorflow:loss = 63.417706, step = 1401 (1.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.5709\n",
      "INFO:tensorflow:loss = 62.880833, step = 1501 (1.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.9339\n",
      "INFO:tensorflow:loss = 62.653446, step = 1601 (1.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.804\n",
      "INFO:tensorflow:loss = 58.140663, step = 1701 (1.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.1869\n",
      "INFO:tensorflow:loss = 63.444916, step = 1801 (1.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.7809\n",
      "INFO:tensorflow:loss = 64.282715, step = 1901 (1.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.3808\n",
      "INFO:tensorflow:loss = 64.75317, step = 2001 (1.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7923\n",
      "INFO:tensorflow:loss = 62.536713, step = 2101 (1.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.5044\n",
      "INFO:tensorflow:loss = 62.441723, step = 2201 (1.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.9655\n",
      "INFO:tensorflow:loss = 58.129555, step = 2301 (1.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.6029\n",
      "INFO:tensorflow:loss = 63.582874, step = 2401 (1.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5045\n",
      "INFO:tensorflow:loss = 56.624756, step = 2501 (1.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.9098\n",
      "INFO:tensorflow:loss = 67.03096, step = 2601 (1.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2885\n",
      "INFO:tensorflow:loss = 63.40699, step = 2701 (1.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.732\n",
      "INFO:tensorflow:loss = 61.881252, step = 2801 (1.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.447\n",
      "INFO:tensorflow:loss = 56.83014, step = 2901 (1.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.3228\n",
      "INFO:tensorflow:loss = 58.48425, step = 3001 (1.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.1516\n",
      "INFO:tensorflow:loss = 57.069145, step = 3101 (1.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.8289\n",
      "INFO:tensorflow:loss = 65.55997, step = 3201 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.2407\n",
      "INFO:tensorflow:loss = 59.424072, step = 3301 (1.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 55.306\n",
      "INFO:tensorflow:loss = 61.923897, step = 3401 (1.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.6563\n",
      "INFO:tensorflow:loss = 58.18686, step = 3501 (1.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.7767\n",
      "INFO:tensorflow:loss = 67.350655, step = 3601 (1.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.991\n",
      "INFO:tensorflow:loss = 66.77133, step = 3701 (1.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2908\n",
      "INFO:tensorflow:loss = 60.862774, step = 3801 (1.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.5564\n",
      "INFO:tensorflow:loss = 63.27567, step = 3901 (1.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.0914\n",
      "INFO:tensorflow:loss = 61.5735, step = 4001 (1.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.0647\n",
      "INFO:tensorflow:loss = 60.941547, step = 4101 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.3037\n",
      "INFO:tensorflow:loss = 63.774548, step = 4201 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4524\n",
      "INFO:tensorflow:loss = 59.624012, step = 4301 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.3779\n",
      "INFO:tensorflow:loss = 61.460426, step = 4401 (1.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.6975\n",
      "INFO:tensorflow:loss = 66.30325, step = 4501 (1.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 55.9576\n",
      "INFO:tensorflow:loss = 60.245037, step = 4601 (1.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7116\n",
      "INFO:tensorflow:loss = 62.84017, step = 4701 (1.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.3057\n",
      "INFO:tensorflow:loss = 60.571754, step = 4801 (1.686 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 56.3576\n",
      "INFO:tensorflow:loss = 59.563095, step = 4901 (1.775 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 58.007576.\n",
      "get Eval Mode dataset (2000, 16)\n",
      "project_essay_2 text_2_vec data shape: (2000, 200)\n",
      "project_essay_1 text_2_vec data shape: (2000, 200)\n",
      "project_resource_summary text_2_vec data shape: (2000, 50)\n",
      "description text_2_vec data shape: (2000, 100)\n",
      "eval dataset length 2000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 0 to 1999\n",
      "Data columns (total 26 columns):\n",
      "id                                              2000 non-null object\n",
      "teacher_id                                      2000 non-null object\n",
      "teacher_prefix                                  2000 non-null object\n",
      "school_state                                    2000 non-null object\n",
      "project_submitted_datetime                      2000 non-null object\n",
      "project_grade_category                          2000 non-null object\n",
      "project_subject_categories                      2000 non-null object\n",
      "project_subject_subcategories                   2000 non-null object\n",
      "project_title                                   2000 non-null object\n",
      "project_essay_1                                 2000 non-null object\n",
      "project_essay_2                                 2000 non-null object\n",
      "project_essay_3                                 87 non-null object\n",
      "project_essay_4                                 87 non-null object\n",
      "project_resource_summary                        2000 non-null object\n",
      "teacher_number_of_previously_posted_projects    2000 non-null int64\n",
      "project_is_approved                             2000 non-null int64\n",
      "teacher_prefix_bins                             2000 non-null int64\n",
      "school_state_bins                               2000 non-null int64\n",
      "project_grade_category_bins                     2000 non-null int64\n",
      "project_subject_categories_bins                 2000 non-null int64\n",
      "project_subject_subcategories_bins              2000 non-null int64\n",
      "posted_projects_periods                         2000 non-null category\n",
      "posted_projects_bins                            2000 non-null int64\n",
      "quantity                                        2000 non-null int64\n",
      "price                                           2000 non-null float64\n",
      "description                                     2000 non-null object\n",
      "dtypes: category(1), float64(1), int64(9), object(15)\n",
      "memory usage: 408.2+ KB\n",
      "eval labels size:2000\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-07-14:55:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-07-14:55:59\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.6255, accuracy_baseline = 0.515, auc = 0.669493, auc_precision_recall = 0.6437211, average_loss = 0.6490424, global_step = 5000, label/mean = 0.485, loss = 64.90424, precision = 0.6202394, prediction/mean = 0.48571864, recall = 0.58762884\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpoi3zg9fx/model.ckpt-5000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "accuracy: 0.6255\n",
      "accuracy_baseline: 0.515\n",
      "auc: 0.669493\n",
      "auc_precision_recall: 0.6437211\n",
      "average_loss: 0.6490424\n",
      "global_step: 5000\n",
      "label/mean: 0.485\n",
      "loss: 64.90424\n",
      "precision: 0.6202394\n",
      "prediction/mean: 0.48571864\n",
      "recall: 0.58762884\n",
      "get Test Mode dataset (78035, 15)\n"
     ]
    }
   ],
   "source": [
    "classifier = train_and_evaluate()\n",
    "\n",
    "result = classifier.predict(input_fn= input_fn(test_file, Test_Mode, num_epochs=1, shuffle=False))\n",
    "\n",
    "predictions = np.array([p['logistic'][0] for p in result])\n",
    "\n",
    "print('predictions.shape', predictions.shape)\n",
    "print(predictions[:10])\n",
    "\n",
    "test_data = pd.read_csv(test_file)\n",
    "predict_result = pd.DataFrame({\n",
    "    \"id\":test_data['id'],\n",
    "    \"project_is_approved\":predictions\n",
    "})\n",
    "predict_result.to_csv('prince_DNN_submission.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
