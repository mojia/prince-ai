{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29999 entries, 0 to 29998\n",
      "Data columns (total 18 columns):\n",
      "id                                              29999 non-null object\n",
      "teacher_id                                      29999 non-null object\n",
      "teacher_prefix                                  29999 non-null object\n",
      "school_state                                    29999 non-null object\n",
      "project_submitted_datetime                      29999 non-null object\n",
      "project_grade_category                          29999 non-null object\n",
      "project_subject_categories                      29999 non-null object\n",
      "project_subject_subcategories                   29999 non-null object\n",
      "project_title                                   29999 non-null object\n",
      "project_essay_1                                 29999 non-null object\n",
      "project_essay_2                                 29999 non-null object\n",
      "project_essay_3                                 994 non-null object\n",
      "project_essay_4                                 994 non-null object\n",
      "project_resource_summary                        29999 non-null object\n",
      "teacher_number_of_previously_posted_projects    29999 non-null int64\n",
      "project_is_approved                             29999 non-null int64\n",
      "posted_projects_bins                            29999 non-null category\n",
      "posted_projects_buckets                         29999 non-null object\n",
      "dtypes: category(1), int64(2), object(15)\n",
      "memory usage: 3.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 18 columns):\n",
      "id                                              30000 non-null object\n",
      "teacher_id                                      30000 non-null object\n",
      "teacher_prefix                                  30000 non-null object\n",
      "school_state                                    30000 non-null object\n",
      "project_submitted_datetime                      30000 non-null object\n",
      "project_grade_category                          30000 non-null object\n",
      "project_subject_categories                      30000 non-null object\n",
      "project_subject_subcategories                   30000 non-null object\n",
      "project_title                                   30000 non-null object\n",
      "project_essay_1                                 30000 non-null object\n",
      "project_essay_2                                 30000 non-null object\n",
      "project_essay_3                                 1096 non-null object\n",
      "project_essay_4                                 1096 non-null object\n",
      "project_resource_summary                        30000 non-null object\n",
      "teacher_number_of_previously_posted_projects    30000 non-null int64\n",
      "project_is_approved                             30000 non-null int64\n",
      "posted_projects_bins                            30000 non-null category\n",
      "posted_projects_buckets                         30000 non-null object\n",
      "dtypes: category(1), int64(2), object(15)\n",
      "memory usage: 3.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78035 entries, 0 to 78034\n",
      "Data columns (total 17 columns):\n",
      "id                                              78035 non-null object\n",
      "teacher_id                                      78035 non-null object\n",
      "teacher_prefix                                  78035 non-null object\n",
      "school_state                                    78035 non-null object\n",
      "project_submitted_datetime                      78035 non-null object\n",
      "project_grade_category                          78035 non-null object\n",
      "project_subject_categories                      78035 non-null object\n",
      "project_subject_subcategories                   78035 non-null object\n",
      "project_title                                   78035 non-null object\n",
      "project_essay_1                                 78035 non-null object\n",
      "project_essay_2                                 78035 non-null object\n",
      "project_essay_3                                 2704 non-null object\n",
      "project_essay_4                                 2704 non-null object\n",
      "project_resource_summary                        78035 non-null object\n",
      "teacher_number_of_previously_posted_projects    78035 non-null int64\n",
      "posted_projects_bins                            78035 non-null category\n",
      "posted_projects_buckets                         78035 non-null object\n",
      "dtypes: category(1), int64(1), object(15)\n",
      "memory usage: 9.6+ MB\n",
      "text_2_vec data shape: (29999, 150)\n",
      "text_2_vec data shape: (30000, 150)\n",
      "text_2_vec data shape: (78035, 150)\n",
      "text_2_vec data shape: (29999, 150)\n",
      "text_2_vec data shape: (30000, 150)\n",
      "text_2_vec data shape: (78035, 150)\n",
      "text_2_vec data shape: (29999, 150)\n",
      "text_2_vec data shape: (30000, 150)\n",
      "text_2_vec data shape: (78035, 150)\n",
      "text_2_vec data shape: (29999, 150)\n",
      "text_2_vec data shape: (30000, 150)\n",
      "text_2_vec data shape: (78035, 150)\n"
     ]
    }
   ],
   "source": [
    "## reference: https://colab.research.google.com/drive/1QhSnbh-WJVGZjQJF8u974msOL_vAgMeS#scrollTo=PlAGuj5kuZm9\n",
    "## https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard import summary as summary_lib\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/DonorsChoose/'\n",
    "train_file='train_1.csv'\n",
    "eval_file='train_2.csv'\n",
    "eval_file='train_2.csv'\n",
    "test_file='test.csv'\n",
    "resource_file='resources.csv'\n",
    "SEED=1000\n",
    "positive_sample_size=30000\n",
    "\n",
    "train_df = pd.read_csv(dir + train_file)\n",
    "eval_df = pd.read_csv(dir + eval_file)\n",
    "test_df = pd.read_csv(dir + test_file)\n",
    "resource_df = pd.read_csv(dir + resource_file)\n",
    "label = LabelEncoder()\n",
    "low_memory=False\n",
    "\n",
    "\n",
    "def sampleData():\n",
    "    train_label_1_df = train_df[train_df['project_is_approved']==1].sample(n=positive_sample_size,\n",
    "                                                                           random_state=SEED)\n",
    "    train_label_0_df = train_df[train_df['project_is_approved']==0]\n",
    "\n",
    "    train_data = pd.concat([train_label_1_df,train_label_0_df])\n",
    "    train_data = shuffle(train_data)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "train_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "eval_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "test_df.fillna(value={\"teacher_prefix\":'Mr.'}, inplace=True)\n",
    "\n",
    "train_data = train_df\n",
    "eval_data = eval_df\n",
    "test_data = test_df \n",
    "\n",
    "\n",
    "def transfer_posted_project_buckets(x):\n",
    "    if x == 0:\n",
    "        return 'L'\n",
    "    elif x ==1:\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'H'\n",
    "\n",
    "all_data = (train_data, eval_data, test_data)\n",
    "for dataset in all_data:\n",
    "    dataset['posted_projects_bins'] = pd.cut(dataset['teacher_number_of_previously_posted_projects'],\n",
    "                                                [-1, 7, 50, 1000])\n",
    "    dataset['posted_projects_buckets'] = label.fit_transform(dataset['posted_projects_bins'])\n",
    "    dataset['posted_projects_buckets'] = dataset['posted_projects_buckets'].apply(transfer_posted_project_buckets)\n",
    "    dataset.info()\n",
    "\n",
    "target = 'project_is_approved'\n",
    "model_dir = tempfile.mkdtemp()\n",
    "vocab_size=10000\n",
    "embedding_size=50\n",
    "sentence_size=150\n",
    "\n",
    "\n",
    "def text_2_vec(feature_name, data_set):\n",
    "    texts = data_set[feature_name].values\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequence_data = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    data = sequence.pad_sequences(sequence_data, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "    print(\"text_2_vec data shape:\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "title_train = text_2_vec('project_title', train_data)\n",
    "title_eval = text_2_vec('project_title', eval_data)\n",
    "title_test = text_2_vec('project_title', test_data)\n",
    "\n",
    "essay_1_train = text_2_vec('project_essay_1', train_data)\n",
    "essay_1_eval = text_2_vec('project_essay_1', eval_data)\n",
    "essay_1_test = text_2_vec('project_essay_1', test_data)\n",
    "\n",
    "essay_2_train = text_2_vec('project_essay_2', train_data)\n",
    "essay_2_eval = text_2_vec('project_essay_2', eval_data)\n",
    "essay_2_test = text_2_vec('project_essay_2', test_data)\n",
    "\n",
    "resource_summary_train = text_2_vec('project_resource_summary', train_data)\n",
    "resource_summary_eval = text_2_vec('project_resource_summary', eval_data)\n",
    "resource_summary_test = text_2_vec('project_resource_summary', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_columns len 9\n",
      "[_IndicatorColumn(categorical_column=_HashedCategoricalColumn(key='teacher_id', hash_bucket_size=1000, dtype=tf.string)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='teacher_prefix', vocabulary_list=('Mrs.', 'Ms.', 'Mr.', 'Teacher', 'Dr.'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='school_state', vocabulary_list=('AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='project_grade_category', vocabulary_list=('Grades 3-5', 'Grades 6-8', 'Grades 9-12', 'Grades PreK-2'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='project_subject_categories', vocabulary_list=('Applied Learning', 'Health & Sports', 'History & Civics', 'Literacy & Language', 'Math & Science', 'Music & The Arts', 'Special Needs', 'Warmth'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='project_subject_subcategories', vocabulary_list=('Applied Learning', 'Care & Hunger', 'Health & Sports', 'History & Civics', 'Literacy & Language', 'Math & Science', 'Music & The Arts', 'Warmth', 'Applied Sciences', 'Character Education', 'Civics & Government', 'College & Career Prep', 'Community Service', 'ESL', 'Early Development', 'Economics', 'Environmental Science', 'Extracurricular', 'Financial Literacy', 'Foreign Languages', 'Gym & Fitness', 'Health & Life Science', 'Health & Wellness', 'History & Geography', 'Literacy', 'Literature & Writing', 'Mathematics', 'Music', 'Nutrition Education', 'Other', 'Parent Involvement', 'Performing Arts', 'Social Sciences', 'Special Needs', 'Team Sports', 'Visual Arts'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='posted_projects_buckets', vocabulary_list=('L', 'M', 'H'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _EmbeddingColumn(categorical_column=_IdentityCategoricalColumn(key='project_title', num_buckets=10000, default_value=None), dimension=50, combiner='mean', layer_creator=<function embedding_column.<locals>._creator at 0x1202781e0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), _EmbeddingColumn(categorical_column=_IdentityCategoricalColumn(key='project_resource_summary', num_buckets=10000, default_value=None), dimension=50, combiner='mean', layer_creator=<function embedding_column.<locals>._creator at 0x1656c9d08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "\n",
    "FEATURES = ['teacher_id','teacher_prefix','school_state','project_grade_category',\n",
    "            'project_subject_categories','project_subject_subcategories',\n",
    "            'posted_projects_buckets'\n",
    "           ]\n",
    "\n",
    "teacher_id = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_hash_bucket('teacher_id', 1000))\n",
    "feature_columns.append(teacher_id)\n",
    "\n",
    "teacher_prefix = tf.feature_column.indicator_column(\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"teacher_prefix\", [\n",
    "        \"Mrs.\",\"Ms.\",\"Mr.\",\"Teacher\",\"Dr.\"\n",
    "    ]))\n",
    "feature_columns.append(teacher_prefix)\n",
    "\n",
    "school_state = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"school_state\",[\n",
    "        \"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\n",
    "        \"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\n",
    "        \"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\n",
    "        \"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"\n",
    "    ]))\n",
    "feature_columns.append(school_state)\n",
    "\n",
    "# TODO project_submitted_datetime\n",
    "\n",
    "project_grade_category = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_grade_category\",[\"Grades 3-5\",\"Grades 6-8\",\"Grades 9-12\",\"Grades PreK-2\"]))\n",
    "feature_columns.append(project_grade_category)\n",
    "\n",
    "project_subject_categories = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_categories\",[\n",
    "       \"Applied Learning\",\"Health & Sports\",\"History & Civics\",\"Literacy & Language\",\n",
    "        \"Math & Science\",\"Music & The Arts\",\"Special Needs\",\"Warmth\"]))\n",
    "feature_columns.append(project_subject_categories)\n",
    "\n",
    "\n",
    "project_subject_subcategories = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"project_subject_subcategories\",[\n",
    "        \"Applied Learning\",\"Care & Hunger\",\"Health & Sports\",\"History & Civics\",\n",
    "        \"Literacy & Language\",\"Math & Science\",\"Music & The Arts\",\n",
    "        \"Warmth\",\"Applied Sciences\",\"Character Education\",\"Civics & Government\",\n",
    "        \"College & Career Prep\",\"Community Service\",\"ESL\",\"Early Development\",\n",
    "        \"Economics\",\"Environmental Science\",\"Extracurricular\",\"Financial Literacy\",\n",
    "        \"Foreign Languages\",\"Gym & Fitness\",\"Health & Life Science\",\"Health & Wellness\",\n",
    "        \"History & Geography\",\"Literacy\",\"Literature & Writing\",\"Mathematics\",\"Music\",\n",
    "        \"Nutrition Education\",\"Other\",\"Parent Involvement\",\"Performing Arts\",\n",
    "        \"Social Sciences\",\"Special Needs\",\"Team Sports\",\"Visual Arts\"]))\n",
    "feature_columns.append(project_subject_subcategories)\n",
    "\n",
    "posted_projects_buckets = tf.feature_column.indicator_column(\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('posted_projects_buckets',\n",
    "                                                              ['L','M','H']))\n",
    "feature_columns.append(posted_projects_buckets)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "project_title = tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity('project_title', vocab_size),\n",
    "                                                  dimension=embedding_size)\n",
    "feature_columns.append(project_title)\n",
    "\n",
    "project_essay_1 = tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity('project_essay_1', vocab_size),\n",
    "                                                     dimension=embedding_size)\n",
    "# feature_columns.append(project_essay_1)\n",
    "\n",
    "project_essay_2 = tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity('project_essay_2', vocab_size),\n",
    "                                                    dimension=embedding_size)\n",
    "# feature_columns.append(project_essay_2)\n",
    "\n",
    "project_resource_summary = tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_identity('project_resource_summary',vocab_size),\n",
    "    dimension=embedding_size)\n",
    "feature_columns.append(project_resource_summary)\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "print('feature_columns len', len(feature_columns))\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x135373da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt-20000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 44.697155, step = 20001\n",
      "INFO:tensorflow:global_step/sec: 13.4451\n",
      "INFO:tensorflow:loss = 45.809395, step = 20101 (7.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.175\n",
      "INFO:tensorflow:loss = 38.825104, step = 20201 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.394\n",
      "INFO:tensorflow:loss = 52.42492, step = 20301 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.051\n",
      "INFO:tensorflow:loss = 33.50102, step = 20401 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.424\n",
      "INFO:tensorflow:loss = 31.460075, step = 20501 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.136\n",
      "INFO:tensorflow:loss = 46.128986, step = 20601 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.534\n",
      "INFO:tensorflow:loss = 30.918348, step = 20701 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.54\n",
      "INFO:tensorflow:loss = 39.274822, step = 20801 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.441\n",
      "INFO:tensorflow:loss = 56.581238, step = 20901 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.301\n",
      "INFO:tensorflow:loss = 35.03737, step = 21001 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.373\n",
      "INFO:tensorflow:loss = 43.032738, step = 21101 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.182\n",
      "INFO:tensorflow:loss = 44.8785, step = 21201 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.3\n",
      "INFO:tensorflow:loss = 45.723804, step = 21301 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.896\n",
      "INFO:tensorflow:loss = 35.668655, step = 21401 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.068\n",
      "INFO:tensorflow:loss = 46.858757, step = 21501 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.203\n",
      "INFO:tensorflow:loss = 37.460506, step = 21601 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.785\n",
      "INFO:tensorflow:loss = 39.65862, step = 21701 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.49\n",
      "INFO:tensorflow:loss = 42.742504, step = 21801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.076\n",
      "INFO:tensorflow:loss = 41.397396, step = 21901 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.218\n",
      "INFO:tensorflow:loss = 43.459152, step = 22001 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.612\n",
      "INFO:tensorflow:loss = 44.229954, step = 22101 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.995\n",
      "INFO:tensorflow:loss = 46.412334, step = 22201 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.513\n",
      "INFO:tensorflow:loss = 39.064255, step = 22301 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.879\n",
      "INFO:tensorflow:loss = 41.41478, step = 22401 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.134\n",
      "INFO:tensorflow:loss = 42.5825, step = 22501 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.097\n",
      "INFO:tensorflow:loss = 32.004337, step = 22601 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.475\n",
      "INFO:tensorflow:loss = 44.19067, step = 22701 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.296\n",
      "INFO:tensorflow:loss = 38.606403, step = 22801 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.39\n",
      "INFO:tensorflow:loss = 39.329018, step = 22901 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.159\n",
      "INFO:tensorflow:loss = 42.290348, step = 23001 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.06\n",
      "INFO:tensorflow:loss = 41.028355, step = 23101 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.501\n",
      "INFO:tensorflow:loss = 42.923298, step = 23201 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.841\n",
      "INFO:tensorflow:loss = 38.236263, step = 23301 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.944\n",
      "INFO:tensorflow:loss = 34.829605, step = 23401 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.926\n",
      "INFO:tensorflow:loss = 26.113441, step = 23501 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.875\n",
      "INFO:tensorflow:loss = 39.430885, step = 23601 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.146\n",
      "INFO:tensorflow:loss = 44.041992, step = 23701 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.296\n",
      "INFO:tensorflow:loss = 41.293987, step = 23801 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.795\n",
      "INFO:tensorflow:loss = 41.175663, step = 23901 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.381\n",
      "INFO:tensorflow:loss = 34.990234, step = 24001 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.188\n",
      "INFO:tensorflow:loss = 34.844215, step = 24101 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.858\n",
      "INFO:tensorflow:loss = 40.762398, step = 24201 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.685\n",
      "INFO:tensorflow:loss = 30.584887, step = 24301 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.554\n",
      "INFO:tensorflow:loss = 46.44081, step = 24401 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.468\n",
      "INFO:tensorflow:loss = 37.633423, step = 24501 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.165\n",
      "INFO:tensorflow:loss = 36.987297, step = 24601 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.233\n",
      "INFO:tensorflow:loss = 32.825745, step = 24701 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.706\n",
      "INFO:tensorflow:loss = 38.246944, step = 24801 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.636\n",
      "INFO:tensorflow:loss = 36.01344, step = 24901 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.313\n",
      "INFO:tensorflow:loss = 40.65823, step = 25001 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.926\n",
      "INFO:tensorflow:loss = 35.932156, step = 25101 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.969\n",
      "INFO:tensorflow:loss = 39.01043, step = 25201 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.452\n",
      "INFO:tensorflow:loss = 40.04321, step = 25301 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.628\n",
      "INFO:tensorflow:loss = 43.37871, step = 25401 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.177\n",
      "INFO:tensorflow:loss = 34.537727, step = 25501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.528\n",
      "INFO:tensorflow:loss = 42.78329, step = 25601 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.607\n",
      "INFO:tensorflow:loss = 43.469067, step = 25701 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.692\n",
      "INFO:tensorflow:loss = 35.439053, step = 25801 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.043\n",
      "INFO:tensorflow:loss = 35.02257, step = 25901 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.216\n",
      "INFO:tensorflow:loss = 39.271816, step = 26001 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.756\n",
      "INFO:tensorflow:loss = 32.242844, step = 26101 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.213\n",
      "INFO:tensorflow:loss = 48.865967, step = 26201 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.523\n",
      "INFO:tensorflow:loss = 36.88933, step = 26301 (0.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.006\n",
      "INFO:tensorflow:loss = 29.280737, step = 26401 (0.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.59\n",
      "INFO:tensorflow:loss = 37.757122, step = 26501 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.11\n",
      "INFO:tensorflow:loss = 32.243126, step = 26601 (0.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.115\n",
      "INFO:tensorflow:loss = 30.250917, step = 26701 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.995\n",
      "INFO:tensorflow:loss = 32.475834, step = 26801 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.336\n",
      "INFO:tensorflow:loss = 41.602955, step = 26901 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 28.137901, step = 27001 (0.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.952\n",
      "INFO:tensorflow:loss = 36.18256, step = 27101 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.474\n",
      "INFO:tensorflow:loss = 39.35386, step = 27201 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.946\n",
      "INFO:tensorflow:loss = 38.632267, step = 27301 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.944\n",
      "INFO:tensorflow:loss = 45.550964, step = 27401 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.817\n",
      "INFO:tensorflow:loss = 44.471027, step = 27501 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.461\n",
      "INFO:tensorflow:loss = 44.091484, step = 27601 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.947\n",
      "INFO:tensorflow:loss = 38.94671, step = 27701 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.217\n",
      "INFO:tensorflow:loss = 41.572014, step = 27801 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.819\n",
      "INFO:tensorflow:loss = 41.510536, step = 27901 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.967\n",
      "INFO:tensorflow:loss = 44.3218, step = 28001 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.344\n",
      "INFO:tensorflow:loss = 31.452085, step = 28101 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.737\n",
      "INFO:tensorflow:loss = 36.46738, step = 28201 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.997\n",
      "INFO:tensorflow:loss = 35.057014, step = 28301 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.406\n",
      "INFO:tensorflow:loss = 41.82775, step = 28401 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.981\n",
      "INFO:tensorflow:loss = 38.82094, step = 28501 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.299\n",
      "INFO:tensorflow:loss = 38.98936, step = 28601 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.062\n",
      "INFO:tensorflow:loss = 37.259613, step = 28701 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.077\n",
      "INFO:tensorflow:loss = 34.196358, step = 28801 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.963\n",
      "INFO:tensorflow:loss = 32.105484, step = 28901 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.272\n",
      "INFO:tensorflow:loss = 35.744896, step = 29001 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.992\n",
      "INFO:tensorflow:loss = 32.75058, step = 29101 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.54\n",
      "INFO:tensorflow:loss = 32.67837, step = 29201 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.967\n",
      "INFO:tensorflow:loss = 34.93914, step = 29301 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.756\n",
      "INFO:tensorflow:loss = 36.52966, step = 29401 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.582\n",
      "INFO:tensorflow:loss = 41.45874, step = 29501 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.419\n",
      "INFO:tensorflow:loss = 39.782955, step = 29601 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.111\n",
      "INFO:tensorflow:loss = 36.038395, step = 29701 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.16\n",
      "INFO:tensorflow:loss = 35.113594, step = 29801 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.171\n",
      "INFO:tensorflow:loss = 36.10707, step = 29901 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.865\n",
      "INFO:tensorflow:loss = 35.404263, step = 30001 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.72\n",
      "INFO:tensorflow:loss = 38.051514, step = 30101 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.503\n",
      "INFO:tensorflow:loss = 39.44194, step = 30201 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.324\n",
      "INFO:tensorflow:loss = 33.817318, step = 30301 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.95\n",
      "INFO:tensorflow:loss = 34.31791, step = 30401 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.411\n",
      "INFO:tensorflow:loss = 38.335022, step = 30501 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.353\n",
      "INFO:tensorflow:loss = 40.556206, step = 30601 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.642\n",
      "INFO:tensorflow:loss = 24.19348, step = 30701 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.63\n",
      "INFO:tensorflow:loss = 35.368393, step = 30801 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.061\n",
      "INFO:tensorflow:loss = 39.496742, step = 30901 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.844\n",
      "INFO:tensorflow:loss = 32.624676, step = 31001 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.861\n",
      "INFO:tensorflow:loss = 33.15752, step = 31101 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.562\n",
      "INFO:tensorflow:loss = 39.9754, step = 31201 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.027\n",
      "INFO:tensorflow:loss = 39.890633, step = 31301 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.68\n",
      "INFO:tensorflow:loss = 47.455276, step = 31401 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.26\n",
      "INFO:tensorflow:loss = 29.99909, step = 31501 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.13\n",
      "INFO:tensorflow:loss = 30.929815, step = 31601 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.012\n",
      "INFO:tensorflow:loss = 43.256607, step = 31701 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.582\n",
      "INFO:tensorflow:loss = 33.26137, step = 31801 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.281\n",
      "INFO:tensorflow:loss = 39.932526, step = 31901 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.872\n",
      "INFO:tensorflow:loss = 37.04499, step = 32001 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.428\n",
      "INFO:tensorflow:loss = 30.794165, step = 32101 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.553\n",
      "INFO:tensorflow:loss = 26.908394, step = 32201 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.803\n",
      "INFO:tensorflow:loss = 39.490154, step = 32301 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.252\n",
      "INFO:tensorflow:loss = 43.545708, step = 32401 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.511\n",
      "INFO:tensorflow:loss = 28.830275, step = 32501 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.007\n",
      "INFO:tensorflow:loss = 40.754555, step = 32601 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.812\n",
      "INFO:tensorflow:loss = 34.29306, step = 32701 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.804\n",
      "INFO:tensorflow:loss = 38.11847, step = 32801 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.81\n",
      "INFO:tensorflow:loss = 31.34592, step = 32901 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.208\n",
      "INFO:tensorflow:loss = 34.62137, step = 33001 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.725\n",
      "INFO:tensorflow:loss = 27.327595, step = 33101 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.124\n",
      "INFO:tensorflow:loss = 37.90636, step = 33201 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.44\n",
      "INFO:tensorflow:loss = 32.48297, step = 33301 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.706\n",
      "INFO:tensorflow:loss = 36.26868, step = 33401 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.045\n",
      "INFO:tensorflow:loss = 32.17821, step = 33501 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.63\n",
      "INFO:tensorflow:loss = 24.523745, step = 33601 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.02\n",
      "INFO:tensorflow:loss = 38.954697, step = 33701 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.892\n",
      "INFO:tensorflow:loss = 32.86623, step = 33801 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.255\n",
      "INFO:tensorflow:loss = 28.509666, step = 33901 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.172\n",
      "INFO:tensorflow:loss = 24.976486, step = 34001 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.975\n",
      "INFO:tensorflow:loss = 36.2389, step = 34101 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.251\n",
      "INFO:tensorflow:loss = 24.899445, step = 34201 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.665\n",
      "INFO:tensorflow:loss = 31.472195, step = 34301 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.806\n",
      "INFO:tensorflow:loss = 37.367077, step = 34401 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.183\n",
      "INFO:tensorflow:loss = 36.10886, step = 34501 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.489\n",
      "INFO:tensorflow:loss = 27.448574, step = 34601 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.934\n",
      "INFO:tensorflow:loss = 34.76189, step = 34701 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.722\n",
      "INFO:tensorflow:loss = 32.203205, step = 34801 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.422\n",
      "INFO:tensorflow:loss = 36.207806, step = 34901 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.455\n",
      "INFO:tensorflow:loss = 39.594727, step = 35001 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.029\n",
      "INFO:tensorflow:loss = 35.826767, step = 35101 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.934\n",
      "INFO:tensorflow:loss = 39.678417, step = 35201 (0.450 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 217.017\n",
      "INFO:tensorflow:loss = 36.195637, step = 35301 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.679\n",
      "INFO:tensorflow:loss = 32.445057, step = 35401 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.972\n",
      "INFO:tensorflow:loss = 30.704779, step = 35501 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.797\n",
      "INFO:tensorflow:loss = 27.934298, step = 35601 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.306\n",
      "INFO:tensorflow:loss = 38.960335, step = 35701 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.589\n",
      "INFO:tensorflow:loss = 37.18516, step = 35801 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.19\n",
      "INFO:tensorflow:loss = 31.476276, step = 35901 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.175\n",
      "INFO:tensorflow:loss = 26.60847, step = 36001 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.789\n",
      "INFO:tensorflow:loss = 30.86615, step = 36101 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.97\n",
      "INFO:tensorflow:loss = 39.082882, step = 36201 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.547\n",
      "INFO:tensorflow:loss = 26.914326, step = 36301 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.323\n",
      "INFO:tensorflow:loss = 41.84975, step = 36401 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.639\n",
      "INFO:tensorflow:loss = 35.348007, step = 36501 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.727\n",
      "INFO:tensorflow:loss = 33.876274, step = 36601 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.341\n",
      "INFO:tensorflow:loss = 30.727642, step = 36701 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.178\n",
      "INFO:tensorflow:loss = 31.210636, step = 36801 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.807\n",
      "INFO:tensorflow:loss = 25.4469, step = 36901 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.437\n",
      "INFO:tensorflow:loss = 36.28245, step = 37001 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.711\n",
      "INFO:tensorflow:loss = 37.55572, step = 37101 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.836\n",
      "INFO:tensorflow:loss = 24.413857, step = 37201 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.639\n",
      "INFO:tensorflow:loss = 33.413418, step = 37301 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.601\n",
      "INFO:tensorflow:loss = 32.543343, step = 37401 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.349\n",
      "INFO:tensorflow:loss = 32.614826, step = 37501 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.798\n",
      "INFO:tensorflow:loss = 33.22561, step = 37601 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.862\n",
      "INFO:tensorflow:loss = 35.615643, step = 37701 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.64\n",
      "INFO:tensorflow:loss = 26.93881, step = 37801 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.049\n",
      "INFO:tensorflow:loss = 33.54493, step = 37901 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.785\n",
      "INFO:tensorflow:loss = 32.376762, step = 38001 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.604\n",
      "INFO:tensorflow:loss = 33.95685, step = 38101 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.766\n",
      "INFO:tensorflow:loss = 38.348736, step = 38201 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.078\n",
      "INFO:tensorflow:loss = 29.544468, step = 38301 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.064\n",
      "INFO:tensorflow:loss = 21.983345, step = 38401 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.529\n",
      "INFO:tensorflow:loss = 33.346516, step = 38501 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.18\n",
      "INFO:tensorflow:loss = 33.28677, step = 38601 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.674\n",
      "INFO:tensorflow:loss = 37.265484, step = 38701 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.011\n",
      "INFO:tensorflow:loss = 36.307907, step = 38801 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.857\n",
      "INFO:tensorflow:loss = 29.549757, step = 38901 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.162\n",
      "INFO:tensorflow:loss = 37.03566, step = 39001 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.823\n",
      "INFO:tensorflow:loss = 30.291628, step = 39101 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.917\n",
      "INFO:tensorflow:loss = 34.179573, step = 39201 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.206\n",
      "INFO:tensorflow:loss = 21.13779, step = 39301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.479\n",
      "INFO:tensorflow:loss = 26.159092, step = 39401 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.802\n",
      "INFO:tensorflow:loss = 26.559992, step = 39501 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.707\n",
      "INFO:tensorflow:loss = 34.676445, step = 39601 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.317\n",
      "INFO:tensorflow:loss = 25.037523, step = 39701 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.661\n",
      "INFO:tensorflow:loss = 27.439043, step = 39801 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.086\n",
      "INFO:tensorflow:loss = 33.44967, step = 39901 (0.456 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 46.311962.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-04-04:53:21\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt-40000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-04-04:53:33\n",
      "INFO:tensorflow:Saving dict for global step 40000: accuracy = 0.815, accuracy_baseline = 0.8486, auc = 0.5451671, auc_precision_recall = 0.8670433, average_loss = 0.5144252, global_step = 40000, label/mean = 0.8486, loss = 51.442524, precision = 0.8516569, prediction/mean = 0.8404446, recall = 0.9469322\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40000: /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt-40000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpehpys581/bow_sparse/model.ckpt-40000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "train_data_features = train_data[FEATURES].values\n",
    "eval_data_features = eval_data[FEATURES].values\n",
    "test_data_features = test_data[FEATURES].values\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n",
    "                                           model_dir=os.path.join(model_dir, 'bow_sparse'))\n",
    "\n",
    "def parser(essay_1, essay_2, resource_summary, title, data_set, y):\n",
    "    features = {k:data_set[:, FEATURES.index(k)] for k in FEATURES}\n",
    "\n",
    "#     features[\"project_essay_1\"] = essay_1\n",
    "#     features[\"project_essay_2\"] = essay_2\n",
    "    features[\"project_resource_summary\"] = resource_summary\n",
    "    features[\"project_title\"] = title\n",
    "    \n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    y_train = train_data[target].values\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_train, essay_2_train,\n",
    "                                                  resource_summary_train, title_train,\n",
    "                                                  train_data_features,\n",
    "                                                  y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    return iterator.get_next()\n",
    "    \n",
    "def eval_input_fn():\n",
    "    y_eval = eval_data[target]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_eval,essay_2_eval,\n",
    "                                                  resource_summary_eval, title_eval,\n",
    "                                                  eval_data_features,\n",
    "                                                  y_eval))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def test_input_fn():\n",
    "    y_test = test_data[target]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_test,essay_2_test,\n",
    "                                                  resource_summary_test, title_test,\n",
    "                                                  test_data_features,\n",
    "                                                  y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps = 20000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(\n",
    "        input_fn=eval_input_fn)])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, \n",
    "                              labels = eval_data[target].astype(bool),\n",
    "                              num_thresholds = 21)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'),\n",
    "                                      sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step = 0)\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 78035\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hz/zfwx8n_d19g70bf5p8jpl4f43zxvln/T/tmpiubdbhgs/bow_sparse/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[0.9698241  0.860133   0.99670154 0.9966903  0.9979532  0.99721354\n",
      " 0.6550113  0.994561   0.9991621  0.18014787]\n"
     ]
    }
   ],
   "source": [
    "def predict_input_fn():\n",
    "    length = len(test_data_features[:, 1])\n",
    "    print('length',length)\n",
    "    y_test = np.empty([length,1])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((essay_1_test,essay_2_test,\n",
    "                                                  resource_summary_test, test_data_features,\n",
    "                                                  y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    \n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "predictions = np.array([p['logistic'][0] for p in classifier.predict(\n",
    "    input_fn=predict_input_fn)])\n",
    "\n",
    "print(predictions[:10])\n",
    "\n",
    "predict_result = pd.DataFrame({\n",
    "    \"id\":test_data['id'],\n",
    "    \"project_is_approved\":predictions\n",
    "})\n",
    "predict_result.to_csv('prince_baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
