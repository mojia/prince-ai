{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resources.csv', 'sample_submission.csv', 'test.csv', 'train.csv', 'train_1.csv', 'train_2.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'teacher_id' 'teacher_prefix' 'school_state'\n",
      " 'project_submitted_datetime' 'project_grade_category'\n",
      " 'project_subject_categories' 'project_subject_subcategories'\n",
      " 'project_title' 'project_essay_1' 'project_essay_2' 'project_essay_3'\n",
      " 'project_essay_4' 'project_resource_summary'\n",
      " 'teacher_number_of_previously_posted_projects' 'project_is_approved']\n"
     ]
    }
   ],
   "source": [
    "# reference https://www.kaggle.com/CVxTz/keras-baseline-feature-hashing-cnn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "dir='/Users/xinwang/ai/dataset/kaggle/DonorsChoose/'\n",
    "\n",
    "print(os.listdir(dir))\n",
    "\n",
    "train = pd.read_csv(dir+'train.csv')\n",
    "test = pd.read_csv(dir+'test.csv')\n",
    "resources = pd.read_csv(dir+'resources.csv')\n",
    "\n",
    "train = train.sort_values(by='project_submitted_datetime')\n",
    "\n",
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of teachers in train:104414, number of teachers in test:55508, overlap:27789\n"
     ]
    }
   ],
   "source": [
    "teachers_train = list(set(train.teacher_id.values))\n",
    "teachers_test = list(set(test.teacher_id.values))\n",
    "\n",
    "inter = set(teachers_train).intersection(teachers_test)\n",
    "\n",
    "print('num of teachers in train:%s, number of teachers in test:%s, overlap:%s' % (len(teachers_train),\n",
    "                                                                                 len(teachers_test),\n",
    "                                                                                 len(inter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: 'id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "/Users/xinwang/ai/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: 'id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    }
   ],
   "source": [
    "char_cols = ['project_subject_categories','project_subject_subcategories','project_title',\n",
    "             'project_essay_1','project_essay_2','project_essay_3','project_essay_4','project_resource_summary']\n",
    "\n",
    "resources.columns\n",
    "resources['total_price'] = resources.quantity * resources.price\n",
    "\n",
    "mean_total_price = pd.DataFrame(resources.groupby('id').total_price.mean())\n",
    "sum_total_price = pd.DataFrame(resources.groupby('id').total_price.sum())\n",
    "count_total_price = pd.DataFrame(resources.groupby('id').total_price.count())\n",
    "\n",
    "mean_total_price['id'] = mean_total_price.index\n",
    "sum_total_price['id'] = sum_total_price.index\n",
    "count_total_price['id'] = count_total_price.index\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    df = pd.merge(df, mean_total_price, on='id')\n",
    "    df = pd.merge(df, sum_total_price, on='id')\n",
    "    df = pd.merge(df, count_total_price, on='id')\n",
    "    \n",
    "    df['year'] = df.project_submitted_datetime.apply(lambda x: x.split('-')[0])\n",
    "    df['month'] = df.project_submitted_datetime.apply(lambda x: x.split('-')[1])\n",
    "    \n",
    "    for col in char_cols:\n",
    "        df[col] = df[col].fillna('NA')\n",
    "\n",
    "    df['text'] = df.apply(lambda x: \" \".join(x[col] for col in char_cols), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = create_features(train)\n",
    "test = create_features(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
       "       'project_submitted_datetime', 'project_grade_category',\n",
       "       'project_subject_categories', 'project_subject_subcategories',\n",
       "       'project_title', 'project_essay_1', 'project_essay_2',\n",
       "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
       "       'teacher_number_of_previously_posted_projects',\n",
       "       'project_is_approved', 'total_price_x', 'total_price_y',\n",
       "       'total_price', 'year', 'month', 'text'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'teacher_id' 'teacher_prefix' 'school_state'\n",
      " 'project_submitted_datetime' 'project_grade_category'\n",
      " 'project_subject_categories' 'project_subject_subcategories'\n",
      " 'project_title' 'project_essay_1' 'project_essay_2' 'project_essay_3'\n",
      " 'project_essay_4' 'project_resource_summary'\n",
      " 'teacher_number_of_previously_posted_projects' 'project_is_approved'\n",
      " 'total_price_x' 'total_price_y' 'total_price' 'year' 'month' 'text'\n",
      " 'teacher_prefix_hash' 'school_state_hash' 'year_hash' 'month_hash'\n",
      " 'project_grade_category_hash' 'project_subject_categories_hash'\n",
      " 'project_subject_subcategories_hash']\n"
     ]
    }
   ],
   "source": [
    "cate_features = ['teacher_prefix', 'school_state','year', 'month','project_grade_category',\n",
    "                 'project_subject_categories', 'project_subject_subcategories']\n",
    "cate_features_hash = [col+'_hash' for col in cate_features]\n",
    "\n",
    "max_size = 15000\n",
    "\n",
    "def feature_hash(df, max_size=max_size):\n",
    "    for col in cate_features:\n",
    "        df[col+'_hash'] = df[col].apply(lambda x: hash(x)%max_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_hash(train)\n",
    "test = feature_hash(test)\n",
    "    \n",
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_words.shape (182080, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "max_features = 50000\n",
    "maxlen = 300\n",
    "\n",
    "numeric_features = ['teacher_number_of_previously_posted_projects','total_price_x', 'total_price_y','total_price']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_numeric = scaler.fit_transform(train[numeric_features])\n",
    "x_test_numeric = scaler.fit_transform(test[numeric_features])\n",
    "print('processed numeric features')\n",
    "\n",
    "x_train_cate = np.array(train[cate_features_hash], dtype=np.int)\n",
    "x_test_cate = np.array(test[cate_features_hash], dtype=np.int)\n",
    "print('processed categorical features')\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(train['text'].tolist() + test['text'].tolist())\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train['text'].tolist())\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test['text'].tolist())\n",
    "\n",
    "x_train_words = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "x_test_words = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "print('processed text features')\n",
    "\n",
    "x_train_target = train.project_is_approved\n",
    "\n",
    "print('x_train_words.shape', x_train_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163872 samples, validate on 18208 samples\n",
      "Epoch 1/5\n",
      "163872/163872 [==============================] - 209s 1ms/step - loss: 0.4076 - acc: 0.8452 - val_loss: 0.3616 - val_acc: 0.8534\n",
      "Epoch 2/5\n",
      "163872/163872 [==============================] - 204s 1ms/step - loss: 0.3619 - acc: 0.8564 - val_loss: 0.3469 - val_acc: 0.8601\n",
      "Epoch 3/5\n",
      "163872/163872 [==============================] - 210s 1ms/step - loss: 0.3376 - acc: 0.8664 - val_loss: 0.3489 - val_acc: 0.8598\n",
      "Epoch 4/5\n",
      "163872/163872 [==============================] - 201s 1ms/step - loss: 0.3114 - acc: 0.8772 - val_loss: 0.3639 - val_acc: 0.8523\n",
      "Epoch 5/5\n",
      "163872/163872 [==============================] - 203s 1ms/step - loss: 0.2783 - acc: 0.8918 - val_loss: 0.3866 - val_acc: 0.8533\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Convolution1D, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input_cate = Input((len(cate_features_hash),))\n",
    "    input_numeric = Input((len(numeric_features),))\n",
    "    input_words = Input((maxlen,))\n",
    "    \n",
    "    x_cate = Embedding(max_size, 10)(input_cate)\n",
    "    x_cate = Flatten()(x_cate)\n",
    "    x_cate = Dropout(0.2)(x_cate)\n",
    "    x_cate = Dense(100, activation='relu')(x_cate)\n",
    "    \n",
    "    x_numeric = Dense(100, activation='relu')(input_numeric)\n",
    "    x_numeric = Dropout(0.2)(x_numeric)\n",
    "    \n",
    "    x_words = Embedding(max_features, 100)(input_words)\n",
    "    x_words = Convolution1D(100, 3, activation='relu')(x_words)\n",
    "    x_words = GlobalMaxPool1D()(x_words)\n",
    "    x_words = Dropout(0.2)(x_words)\n",
    "    \n",
    "    x = concatenate([x_cate, x_numeric, x_words])\n",
    "    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_cate,input_numeric,input_words], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(0.001, decay=1e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = get_model()\n",
    "model.fit([x_train_cate,x_train_numeric,x_train_words],x_train_target, validation_split=0.1, epochs=5, batch_size=1024)\n",
    "predict_test = model.predict([x_test_cate,x_test_numeric,x_test_words])\n",
    "\n",
    "\n",
    "test['project_is_approved'] = predict_test\n",
    "test[['id','project_is_approved']].to_csv('baseline_keras_nn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
